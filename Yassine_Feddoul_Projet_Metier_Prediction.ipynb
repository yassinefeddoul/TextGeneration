{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TextGeneration.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"ZTI8UsQ8afxe"},"source":["from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras import regularizers\n","import tensorflow.keras.utils as ku \n","import numpy as np "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0-G--MnRblJX"},"source":["from google.colab import drive"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tWoUzsifbmJ-","outputId":"a16cdada-9155-4307-d345-d6da6fb21080"},"source":["drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","metadata":{"id":"NQ_sTrd-boY-"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5OpRcIq2aglg","outputId":"3bbd48ed-2307-4598-d562-bd5895953e0c"},"source":["tokenizer = Tokenizer()\n","\n","# sonnets.txt\n","!gdown --id 108jAePKK4R3BVYBbYJZ32JWUwxeMg20K\n","\n","#data = open('./Projet_Metier.txt').read()\n","data = \"\"\"\n","La plupart des accidents de la route sont dus √† une erreur humaine, les syst√®mes avanc√©s d'aide √† la conduite (ADAS) sont des syst√®mes d√©velopp√©s pour automatiser, adapter et am√©liorer le v√©hicule pour plus de s√©curit√© et une meilleure conduite.\n","Le syst√®me automatis√© qui est fourni par l'ADAS au v√©hicule a prouv√© qu'il permettait de r√©duire le nombre de d√©c√®s et des d√©g√¢ts d‚Äôune mani√®re remarquable sur les routes, en minimisant les erreurs humaines.\n","Les dispositifs de s√©curit√© sont con√ßus pour √©viter les collisions et les accidents en proposant des technologies qui alertent le conducteur en cas de probl√®me potentiel ou pour √©viter les collisions en mettant en place des mesures de protection et en prenant le contr√¥le du v√©hicule.\n","Nous souhaitons d√©ployer un syst√®me d‚Äôassistance √† la conduite ADAS (Advanced Driving Assistance System) qui s'appuie sur des technologies de l'intelligence artificielle.\n","Pour subvenir √† ces besoins, il est primordial de passer par la pr√©diction des trajectoires des objets mobiles.\n","Dans la premi√®re partie du projet nous avons vu de mani√®re globale les syst√®mes d‚Äôaide √† la conduite pour les v√©hicules autonomes, nous avons vu que ces v√©hicules n√©cessitent l‚Äôinteractions avec leur entourage, que ce soit √† la suite d'un lent v√©hicule, se coordonnant pour se relayer avec les v√©hicules aux intersections, ou man≈ìuvrer autour d'autres v√©hicules pour atteindre le stationnement spots, il est presque impossible de faire un voyage en voiture sans √™tre affect√© par un autre v√©hicule d'une mani√®re ou d'une autre.\n","Les syst√®mes d'assistance et les v√©hicules autonomes deviennent plus sophistiqu√©s, le raisonnement sur ces interactions avec les v√©hicules devient de plus en plus important.\n","Notre r√¥le consiste √† manifeste √† pr√©dire les trajectoires des objets mobiles, en se basant sur l‚Äôensemble d‚Äôinformation collect√©es a l‚Äôinterm√©diaire des capteurs, cameras, qui vont √™tre traiter √† la suite pour aider √† d√©terminer l‚Äôensembles de d√©cisions futures que l‚Äôautomobile va prendre. \n","Nous savons probablement tous que les ordinateurs et les algorithmes s'am√©liorent chaque jour pour \"penser\", analyser des situations et prendre des d√©cisions comme le font les humains. La compr√©hension de la vision par ordinateur fait partie int√©grante de ces progr√®s dans le domaine de l'intelligence artificielle. Il y a 10 ans, la d√©tection des objets √©tait un myst√®re pour nous, il n'√©tait m√™me pas utile de mentionner le suivi des objets. Ces technologies ont fait de grands progr√®s √† cet √©gard et les limites sont remises en question et repouss√©es au fur et √† mesure que nous parlons.\n","Le syst√®me d‚Äôaide √† la conduite ou syst√®me avanc√© d‚Äôaide √† la conduite est un syst√®me de s√©curit√© active d'information ou d'assistance du conducteur pour √©viter l'apparition d'une situation dangereuse risquant d'aboutir √† un accident.\n","Si la d√©tection d'objets dans une image a fait l'objet d'une grande attention de la part de la communaut√© scientifique, un domaine moins connu et pourtant aux applications tr√®s r√©pandues est le suivi d'objets dans une vid√©o ou un flux vid√©o en temps r√©el. Il s'agit d'un domaine qui n√©cessite de fusionner nos connaissances en mati√®re de d√©tection d'objets dans les images avec l'analyse des informations temporelles et leur utilisation pour pr√©dire les trajectoires des mouvements.\n","Le suivi d'objets peut √™tre utilis√© dans le cadre d'√©v√©nements sportifs, par exemple pour le suivi d'un match de basket-ball, pour attraper des cambrioleurs, pour compter les voitures qui passent dans la rue ou m√™me pour rep√©rer le moment o√π votre chien court √† l'ext√©rieur de la maison. Il existe de nombreux domaines dans lesquels nous pouvons utiliser le suivi d'objets.\n","Les syst√®mes d'assistance √† la conduite augmentent la s√©curit√© et la rentabilit√© des voitures, des utilitaires et des v√©hicules sp√©ciaux et sont des √©l√©ments cl√©s de la mobilit√© de demain. Ils incluent des applications modernes telles que l'aide √† la marche arri√®re, le r√©gulateur de vitesse adaptatif (ACC), la reconnaissance des panneaux de signalisation, les assistants de changement de direction, d‚Äôangle mort et de maintien de trajectoire ou encore la vue panoramique √† 360¬∞, lesquelles ne sauraient se passer de composants tels que des cam√©ras robustes, des capteurs optiques et des syst√®mes LIDAR. First Sensor fournit √† des √©quipementiers de premi√®re monte, des int√©grateurs et des √©quipementiers de deuxi√®me monte op√©rant √† diff√©rents niveaux de la cha√Æne de cr√©ation de valeur des produits allant du composant au module et au syst√®me complet.\n","L'object tracker, quant √† lui, doit suivre un objet particulier sur toute la s√©rie d'images (par exemple une vid√©o). Si le d√©tecteur d√©tecte deux bananes dans l'image, le tracker d'objets doit identifier les deux d√©tections distinctes et doit les suivre dans les images suivantes (√† l'aide d'un identifiant d'objet unique).\n","L'occlusion d'objets dans les vid√©os est l'un des obstacles les plus courants au suivi des objets. Comme vous pouvez le voir, dans la figure ci-dessus (√† gauche), l'homme √† l'arri√®re- plan est d√©tect√©, alors que le m√™me homme n'est pas d√©tect√© dans l'image suivante (√† droite). Maintenant, le d√©fi pour le traqueur consiste √† identifier le m√™me homme lorsqu'il est d√©tect√© dans une image beaucoup plus tard et √† associer son ancienne trace et ses caract√©ristiques √† sa trajectoire.\n","Notre projet a comme objectif de d√©tecter et suivre les objets mobiles afin de pr√©dire leurs comportements futurs.\n","En r√©sum√© ce rapport se compose de deux chapitres, le premier r√©sume la bibliographie n√©cessaire pour la compr√©hension pr√©liminaire du sujet et pouvoir entamer notre \n","sujet, le deuxi√®me concerne la partie pratique de notre projet o√π on a impl√©ment√© les diff√©rentes solutions r√©solvant notre probl√©matique, faire une √©tude comparative ainsi conclure la solution la plus ad√©quate pour notre projet.\n","Comme nous sommes dans la premi√®re phase d'un projet de grande envergure, ce rapport fournit √©galement une vaste bibliographie pour le sujet discut√©.\n","La plupart des accidents de la route sont dus √† une erreur humaine, les syst√®mes avanc√©s d'aide √† la conduite (ADAS) sont des syst√®mes d√©velopp√©s pour automatiser, adapter et am√©liorer le v√©hicule pour plus de s√©curit√© et une meilleure conduite. \n","Le syst√®me automatis√© qui est fourni par l'ADAS au v√©hicule a prouv√© qu'il permettait de r√©duire le nombre de d√©c√®s et des d√©g√¢ts d‚Äôune mani√®re remarquable sur les routes, en minimisant les erreurs humaines. Les dispositifs de s√©curit√© sont con√ßus pour √©viter les collisions et les accidents en proposant des technologies qui alertent le conducteur en cas de \n","probl√®me potentiel ou pour √©viter les collisions en mettant en place des mesures de protection \n","et en prenant le contr√¥le du v√©hicule.\n","Nous souhaitons d√©ployer un syst√®me d‚Äôassistance √† la conduite ADAS (Advanced Driving Assistance System) qui s'appuie sur des technologies de l'intelligence artificielle. \n","Pour subvenir √† ces besoins, il est primordial de passer par la pr√©diction des trajectoires des objets mobiles\n","La conduite dans un environnement urbain n√©cessite de l‚Äôinteraction avec d‚Äôautres v√©hicules, que ce soit √† la suite d'un lent v√©hicule, se coordonnant pour se relayer avec les v√©hicules aux intersections, ou man≈ìuvrer autour d'autres v√©hicules pour atteindre le stationnement spots, il est presque impossible de faire un voyage en voiture sans √™tre affect√© par un autre v√©hicule d'une mani√®re ou d'une autre.\n","En tant que conducteur, les syst√®mes d'assistance et les v√©hicules autonomes \n","deviennent plus sophistiqu√©, le raisonnement sur ces interactions avec les v√©hicules \n","deviennent de plus en plus importants. Dont notre r√¥le commence, qui se manifeste √† pr√©dire \n","les trajectoires des objets mobiles, en se basant sur l‚Äôensemble d‚Äôinformation collect√©es a \n","l‚Äôinterm√©diaire des capteurs, cameras ..., qui vont √™tre traiter √† la suite pour aider √† d√©terminer \n","l‚Äôensembles de d√©cisions futur que l‚Äôautomobile va prendre.\n","Les syst√®mes avanc√©s d'aide √† la conduite sont des syst√®mes intelligents qui r√©sident √† \n","l'int√©rieur du v√©hicule et assistent le conducteur principal de diverses mani√®res. Ces syst√®mes \n","peuvent √™tre utilis√©s pour fournir des informations vitales sur le trafic, la fermeture et le \n","blocage des routes, les niveaux d'embouteillage, les itin√©raires sugg√©r√©s pour √©viter les \n","embouteillages, etc.\n","Pour donner une id√©e de ce que les syst√®mes d'aide √† la conduite repr√©sentent pour les \n","utilisateurs, nous pr√©sentons un aper√ßu des technologies existantes. Pour des raisons de \n","commodit√©, elles ont √©t√© divis√©es en sous-cat√©gories. Ce bref aper√ßu de la technologie ADAS \n","existante ne met en √©vidence que les types d'ADAS les plus \"courants\".\n","La plupart des accidents de la route sont dus √† une erreur humaine, les syst√®mes avanc√©s d'aide √† la conduite (ADAS) sont des syst√®mes d√©velopp√©s pour automatiser, adapter et am√©liorer le v√©hicule pour plus de s√©curit√© et une meilleure conduite.\n","Le syst√®me automatis√© qui est fourni par l'ADAS au v√©hicule a prouv√© qu'il permettait de r√©duire le nombre de d√©c√®s et des d√©g√¢ts d‚Äôune mani√®re remarquable sur les routes, en minimisant les erreurs humaines. Les dispositifs de s√©curit√© sont con√ßus pour √©viter les collisions et les accidents en proposant des technologies qui alertent le conducteur en cas de probl√®me potentiel ou pour √©viter les collisions en mettant en place des mesures de protection et en prenant le contr√¥le du v√©hicule.\n","Nous souhaitons d√©ployer un syst√®me d‚Äôassistance √† la conduite ADAS (Advanced Driving Assistance System) qui s'appuie sur des technologies de l'intelligence artificielle. Pour subvenir √† ces besoins, il est primordial de passer par la pr√©diction des trajectoires des objets mobiles.\n","Pour cadrer notre travaille nous avons √©laborer un cahier de charge qui se compose de 4 axes principales\n","contexte et pr√©sentation du projet, dont on a d√©fini de fa√ßon br√®ve notre projet,\n","analyse du besoin, o√π on a exprim√© l‚Äôutilit√© de notre projet,\n","Expression fonctionnelle du besoin o√π les contraintes et les fonctionnalit√©s de notre syst√®me sont d√©crites,\n","Planning de r√©alisation.\n","Pour aboutir √† cet objectif, nous devons Tracker les objets mobiles et Pr√©dire les trajectoires des objets mobiles.\n","Dans la premi√®re partie du projet nous avons vu de mani√®re globale les syst√®mes d‚Äôaide √† la conduite pour les v√©hicules autonomes, nous avons vu que ces v√©hicules n√©cessitent l‚Äôinteractions avec leur entourage, que ce soit √† la suite d'un lent v√©hicule, se coordonnant pour se relayer avec les v√©hicules aux intersections, ou man≈ìuvrer autour d'autres v√©hicules pour atteindre le stationnement spots, il est presque impossible de faire un voyage en voiture sans √™tre affect√© par un autre v√©hicule d'une mani√®re ou d'une autre.\n","Les syst√®mes d'assistance et les v√©hicules autonomes deviennent plus sophistiqu√©s, le raisonnement sur ces interactions avec les v√©hicules devient de plus en plus importants.\n","Notre r√¥le consiste √† manifeste √† pr√©dire les trajectoires des objets mobiles, en se basant sur l‚Äôensemble d‚Äôinformation collect√©es a l‚Äôinterm√©diaire des capteurs, cameras ..., qui vont √™tre traiter √† la suite pour aider √† d√©terminer l‚Äôensembles de d√©cisions futur que l‚Äôautomobile va prendre. \n","Nous savons probablement tous que les ordinateurs et les algorithmes s'am√©liorent chaque jour pour \"penser\", analyser des situations et prendre des d√©cisions comme le font les humains. La compr√©hension de la vision par ordinateur fait partie int√©grante de ces progr√®s dans le domaine de l'intelligence artificielle. Il y a 10 ans, la d√©tection des objets √©tait un myst√®re pour nous, il n'√©tait m√™me pas utile de mentionner le suivi des objets. Ces technologies ont fait de grands progr√®s √† cet √©gard et les limites sont remises en question et repouss√©es au fur et √† mesure que nous parlons.\n","Si la d√©tection d'objets dans une image a fait l'objet d'une grande attention de la part de la communaut√© scientifique, un domaine moins connu et pourtant aux applications tr√®s r√©pandues est le suivi d'objets dans une vid√©o ou un flux vid√©o en temps r√©el. Il s'agit d'un domaine qui n√©cessite de fusionner nos connaissances en mati√®re de d√©tection d'objets dans les images avec l'analyse des informations temporelles et leur utilisation pour pr√©dire les trajectoires des mouvements.\n","Le suivi d'objets peut √™tre utilis√© dans le cadre d'√©v√©nements sportifs, par exemple pour le suivi d'un match de basket-ball, pour attraper des cambrioleurs, pour compter les voitures qui passent dans la rue ou m√™me pour rep√©rer le moment o√π votre chien court √† l'ext√©rieur de la maison. Il existe de nombreux domaines dans lesquels nous pouvons utiliser le suivi d'objets.\n","En termes simples, dans la d√©tection d'objets, nous d√©tectons un objet dans une image, nous l'entourons d'un bounding box ou d'un masque et nous le classons. Notez que le travail du d√©tecteur s'arr√™te l√†. Il traite chaque image ind√©pendamment et identifie de nombreux objets dans cette image particuli√®re.\n","L'object tracker, quant √† lui, doit suivre un objet particulier sur toute la s√©rie d'images (par exemple une vid√©o). Si le d√©tecteur d√©tecte deux bananes dans l'image, le tracker d'objets doit identifier les deux d√©tections distinctes et doit les suivre dans les images suivantes (√† l'aide d'un identifiant d'objet unique).\n","L'occlusion d'objets dans les vid√©os est l'un des obstacles les plus courants au suivi des objets. Comme vous pouvez le voir, dans la figure ci-dessus (√† gauche), l'homme √† l'arri√®re- plan est d√©tect√©, alors que le m√™me homme n'est pas d√©tect√© dans l'image suivante (√† droite). Maintenant, le d√©fi pour le traqueur consiste √† identifier le m√™me homme lorsqu'il est d√©tect√© dans une image beaucoup plus tard et √† associer son ancienne trace et ses caract√©ristiques √† sa trajectoire.\n","Dans des exemples r√©els, la t√¢che consisterait √† suivre un objet √† travers diff√©rentes cam√©ras, ce qui peut √™tre utilis√© dans des magasins d'IA o√π il n'y a pas de caissier et o√π nous devons suivre un client dans tout le magasin. Un exemple similaire est illustr√© ci-dessus. En cons√©quence, il y aura des changements significatifs dans la fa√ßon dont nous voyons l'objet\n","dans chaque cam√©ra. Dans de tels cas, les caract√©ristiques utilis√©es pour suivre un objet deviennent tr√®s importantes car nous devons nous assurer qu'elles sont invariantes aux changements de vues.\n","Lorsque la cam√©ra utilis√©e pour le suivi d'un objet particulier est √©galement en mouvement pour cet objet, cela peut souvent entra√Æner des cons√©quences inattendues. Comme je l'ai mentionn√© pr√©c√©demment, de nombreux trackers prennent en compte les caract√©ristiques d'un objet pour le suivre. Un tel tracker peut √©chouer dans des sc√©narios o√π l'objet appara√Æt diff√©rent √† cause du mouvement de la cam√©ra (il appara√Æt plus grand ou plus petit). Un tracker robuste pour ce probl√®me peut √™tre tr√®s utile dans des applications importantes comme les drones de suivi d'objets ou la navigation autonome.\n","L'une des choses les plus ennuyeuses dans la construction d'un tracker d'objets est d'obtenir de bonnes donn√©es d'entra√Ænement pour un sc√©nario particulier. Contrairement √† la construction d'un ensemble de donn√©es pour un d√©tecteur d'objets, nous pouvons avoir des images non connect√©es de mani√®re al√©atoire. Dans le suivi d'objets, il est n√©cessaire de disposer de s√©quences vid√©o ou d'images o√π chaque instance de l'objet est identifi√©e de mani√®re unique pour chaque image.\n","Mean-shift or Mode seeking est un algorithme populaire, principalement utilis√© dans le clustering et d'autres probl√®mes non supervis√©s connexes. Il est similaire √† K-Means mais remplace la technique simple du centro√Ød pour calculer les centres des clusters par une moyenne pond√©r√©e qui donne de l'importance aux points qui sont plus proches de la moyenne. L'objectif de l'algorithme est de trouver tous les modes dans la distribution de\n","donn√©es donn√©e. De plus, cet algorithme ne requiert pas une valeur \"K\" optimale comme K-Means\n","Supposons que nous ayons la d√©tection d'un objet dans l'image et que nous extrayons certaines caract√©ristiques de la d√©tection (couleur, texture, histogramme, etc.). En appliquant l'algorithme de Mean-shift, nous avons une id√©e g√©n√©rale de l'endroit o√π se trouve le mode de la distribution des caract√©ristiques dans l'√©tat actuel. Maintenant, lorsque nous avons l'image suivante, o√π cette distribution a chang√© en raison du mouvement de l'objet dans l'image, l'algorithme de Mean-shift recherche le nouveau mode le plus important et suit donc l'objet.\n","Cette m√©thode diff√®re du mean-shift, car nous n'utilisons pas n√©cessairement les caract√©ristiques extraites de l'objet d√©tect√©. Au lieu de cela, l'objet est suivi en utilisant les variations spatio-temporelles de la luminosit√© de l'image au niveau des pixels.\n","Nous nous concentrons ici sur l'obtention d'un vecteur de d√©placement pour l'objet √† suivre √† travers les images.\n","Dans presque tous les probl√®mes d'ing√©nierie qui impliquent une pr√©diction dans un sens temporel ou de s√©rie temporelle, qu'il s'agisse de vision par ordinateur, de guidage, de navigation, de syst√®mes de stabilisation ou m√™me d'√©conomie, vous entendrez souvent parler de \"filtre de Kalman\".\n","L'id√©e centrale d'un filtre de Kalman est d'utiliser les d√©tections disponibles et les pr√©dictions pr√©c√©dentes pour arriver √† la meilleure estimation de l'√©tat actuel tout en gardant la possibilit√© d'erreurs dans le processus.\n","Comme nous le voyons dans l'image ci-dessus, le filtre de Kalman fonctionne de mani√®re r√©cursive, o√π nous prenons les mesures actuelles, pour pr√©dire l'√©tat actuel, puis nous utilisons les mesures et mettons √† jour nos pr√©dictions. Donc, cela revient essentiellement √† d√©duire une nouvelle distribution (les pr√©dictions) √† partir de la distribution de l'√©tat pr√©c√©dent et de la distribution des mesures.\n","Par exemple, nous pouvons maintenant entra√Æner un assez bon d√©tecteur d'objets YOLOv3 qui d√©tecte une personne. Mais il n'est pas tr√®s pr√©cis et rate occasionnellement des d√©tections, disons 10% des images. Pour suivre et pr√©dire efficacement le prochain √©tat d'une personne, supposons un \"mod√®le de vitesse constante\". Maintenant, une fois que nous avons d√©fini le mod√®le simple selon les lois de la physique, √©tant donn√© une d√©tection actuelle, nous\n","pouvons faire une bonne estimation de l'endroit o√π la personne sera dans la prochaine image. Tout cela semble parfait, dans un monde id√©al, mais il y a toujours une composante de bruit.\n","L'une des premi√®res m√©thodes qui a utilis√© l'apprentissage profond, pour le suivi d'un seul objet, √©tait les r√©seaux de r√©gression profonde. Un mod√®le est entra√Æn√© sur un dataset compos√© de vid√©os avec des images cibles √©tiquet√©es. L'objectif du mod√®le est de simplement suivre un objet donn√© √† partir du cadre de l'image donn√©e.\n","Pour y parvenir, ils utilisent une architecture CNN √† deux frames qui utilise √† la fois le frame actuel et le frame pr√©c√©dente pour r√©gresser avec pr√©cision sur l'objet.\n","Comme le montre la figure ci-dessus, nous prenons le cadre de l'image pr√©c√©dente en\n","fonction des pr√©dictions et d√©finissons une \"r√©gion de recherche\" dans l'image actuelle en fonction de ce cadre. Le r√©seau est ensuite entra√Æn√© √† r√©gresser pour trouver l'objet dans cette r√©gion de recherche. L'architecture du r√©seau est simple avec des CNN suivis de Fully connected layers qui nous donnent directement les coordonn√©es du Bounding Box. \n","ROLO \"Recurrent Yolo\" est une m√©thode √©l√©gante de suivre des objets en utilisant le Deep CNN. C'est un d√©tecteur YOLO l√©g√®rement modifi√© avec une unit√© LSTM r√©currente attach√©e √† la fin, qui aide √† suivre les objets en capturant les caract√©ristiques spatio-temporelles.\n","Comme le montre l'image ci-dessus, l'architecture est assez simple. Les d√©tections de YOLO (bounding boxes) sont concat√©n√©es avec le vecteur de caract√©ristiques d'un extracteur de caract√©ristiques bas√© sur CNN (nous pouvons soit r√©utiliser le backend de YOLO, soit utiliser un extracteur de caract√©ristiques sp√©cialis√©). Maintenant, ce vecteur de caract√©ristiques concat√©n√©, qui repr√©sente la plupart des informations spatiales li√©es √† l'objet actuel, ainsi que les informations sur l'√©tat pr√©c√©dent, est transmis √† la cellule LSTM.\n","La sortie de la cellule tient maintenant compte des informations spatiales et temporelles. Cette simple astuce consistant √† utiliser des CNN pour l'extraction de caract√©ristiques et des LSTM pour les pr√©dictions de la bo√Æte englobante a permis d'am√©liorer consid√©rablement les probl√®mes de suivi. L'inconv√©nient de cette m√©thode est qu'il est assez difficile d'entra√Æner le r√©seau LSTM. \n","Le framework de suivi d'objets le plus populaire et l'un des plus utilis√©s est le Deep SORT (Simple Real-time Tracker).\n","Dans le tracker Deep Sort, le filtre de Kalman est un composant crucial. Le sc√©nario de suivi de Kalman est d√©fini sur l'espace d'√©tat √† huit dimensions (u, v, a, h, u', v', a', h') qui contient la position du centre de la bo√Æte englobante (u, v), sont les centres des bo√Ætes englobantes, a est le rapport d'aspect et h, la hauteur de l'image. Les autres variables sont les vitesses respectives des variables.\n","Notez que les variables n'ont que des facteurs de position et de vitesse absolus puisque nous supposons un mod√®le de vitesse lin√©aire simple. Le filtre de Kalman nous aide √† prendre en compte le bruit dans la d√©tection et utilise l'√©tat ant√©rieur pour pr√©dire un bon ajustement des bo√Ætes englobantes.\n","Pour chaque d√©tection, nous cr√©ons une \"piste\", qui contient toutes les informations d'√©tat n√©cessaires. Elle poss√®de √©galement un param√®tre permettant de suivre et de supprimer les pistes dont la derni√®re d√©tection r√©ussie remonte √† longtemps, car ces objets auraient quitt√© la sc√®ne. Pour √©liminer les pistes en double, il existe un seuil de nombre minimum de d√©tections pour les premi√®res images.\n","Nous allons commencer par comprendre les SVM en termes simples. \n","La ligne s√©pare assez bien les classes. C'est ce que fait essentiellement le SVM - une simple s√©paration des classes. Le SVM, essaie de trouver une ligne/hyperplan (dans un espace multidimensionnel) qui s√©pare ces deux classes. Ensuite, il classe le nouveau point selon qu'il se trouve du c√¥t√© positif ou n√©gatif de l'hyperplan en fonction des classes √† pr√©dire.\n","La r√©gression par vecteurs de support (SVR) utilise le m√™me principe que le SVM, mais pour les probl√®mes de r√©gression\n","Consid√©rons que ces deux lignes rouges comme la limite de d√©cision et la ligne verte comme l'hyperplan. Notre objectif, lorsque nous avan√ßons avec le SVR, est de consid√©rer essentiellement les points qui sont √† l'int√©rieur de la ligne de limite de d√©cision. Notre ligne de meilleur ajustement est l'hyperplan qui a un nombre maximum de points.\n","La premi√®re chose que nous allons comprendre est ce qu'est la fronti√®re de d√©cision (la dangereuse ligne rouge ci-dessus !). Consid√©rez que ces lignes se trouvent √† une distance quelconque, disons 'a', de l'hyperplan. Ce sont donc les lignes que nous tra√ßons √† la distance '+a' et '-a' de l'hyperplan\n","Le perceptron multicouche (MLP) est un compl√©ment du r√©seau neuronal √† action directe. Il se compose de trois types de couches : la couche d'entr√©e, la couche de sortie et la couche cach√©e, comme le montre la figure. La couche d'entr√©e re√ßoit le signal d'entr√©e √† traiter. La t√¢che requise, telle que la pr√©diction et la classification, est ex√©cut√©e par la couche de sortie. Un nombre arbitraire de couches cach√©es, plac√©es entre la couche d'entr√©e et la couche de sortie, constitue le v√©ritable moteur de calcul du MLP. Comme dans le cas d'un r√©seau de type feed forward, dans un MLP, les donn√©es circulent dans le sens direct de la couche d'entr√©e √† la couche de sortie. Les neurones du MLP sont form√©s avec l'algorithme d'apprentissage par r√©tro propagation. Les MLP sont con√ßus pour approximer toute fonction continue et peuvent r√©soudre des probl√®mes qui ne sont pas lin√©airement s√©parables. Les principaux cas d'utilisation des MLP sont la classification, la reconnaissance, la pr√©diction et\n","l'approximation des formes.\n","Couche d'entr√©e Cette couche accepte les caract√©ristiques d'entr√©e. Elle fournit au r√©seau des informations provenant du monde ext√©rieur. Aucun calcul n'est effectu√© dans cette couche, les n≈ìuds transmettent simplement les informations (caract√©ristiques) √† la couche cach√©e.\n","Couche cach√©e Les n≈ìuds de cette couche ne sont pas expos√©s au monde ext√©rieur, ils font partie de l'abstraction fournie par tout r√©seau neuronal. La couche cach√©e effectue\n","toutes sortes de calculs sur les caract√©ristiques entr√©es par la couche d'entr√©e et transf√®re le r√©sultat √† la couche de sortie.\n","Couche de sortie : Cette couche transmet les informations apprises par le r√©seau au monde ext√©rieur.\n","Fonction d‚Äôactivation La fonction d'activation d√©cide si un neurone doit √™tre activ√© ou non en calculant la somme pond√©r√©e et en y ajoutant un biais. Le but de la fonction d'activation est d'introduire une non- lin√©arit√© dans la sortie d'un neurone.\n","Nous savons qu'un r√©seau neuronal poss√®de des neurones qui fonctionnent en correspondance avec un poids, un biais et leur fonction d'activation respective. Dans un r√©seau neuronal, nous mettons √† jour les poids et les biais des neurones sur la base de l'erreur √† la sortie. Ce processus est connu sous le nom de r√©tro propagation. Les fonctions d'activation rendent la r√©tro propagation possible puisque les gradients sont fournis avec l'erreur pour mettre √† jour les poids et les biais.\n","Exemples de fonction d‚Äôactivation : ¬´ Sigmoid - Tanh - RELU - Softmax ‚Äì Linear ¬ª\n","Pourquoi la fonction d‚Äôactivation\n","Un r√©seau neuronal sans fonction d'activation est essentiellement un simple mod√®le de r√©gression lin√©aire. La fonction d'activation effectue la transformation non lin√©aire de l'entr√©e, ce qui le rend capable d'apprendre et d'ex√©cuter des t√¢ches plus complexes.\n","L'analyse de r√©gression est une technique de mod√©lisation pr√©dictive qui estime la relation entre deux ou plusieurs variables, il se concentre sur la relation entre une variable d√©pendante (cible) et une ou plusieurs variables ind√©pendantes (pr√©dicteurs). Ici, la variable d√©pendante est suppos√©e √™tre l'effet de la ou des variables ind√©pendantes. La valeur des pr√©dicteurs est utilis√©e pour estimer ou pr√©dire la valeur probable de la variable cible.\n","Par exemple, pour d√©crire la relation entre la consommation de diesel et la production industrielle, si l'on suppose que la \"consommation de diesel\" est l'effet de la \"production industrielle\", nous pouvons effectuer une analyse de r√©gression pour pr√©dire la valeur de la \"consommation de diesel\" pour une valeur sp√©cifique de la \"production industrielle\".\n","Etapes pour effectuer une r√©gression lin√©aire Supposer une relation math√©matique entre la cible et le(s) pr√©dicteur(s). \"La relation peut √™tre une ligne droite (r√©gression lin√©aire) ou une courbe polynomiale (r√©gression polynomiale) ou une relation non lin√©aire (r√©gression non lin√©aire)\". Cr√©ez un diagramme de dispersion de la variable cible et de la variable pr√©dictive (m√©thode la plus simple et la plus populaire).\n","En g√©n√©rale, l'analyse de r√©gression comprend l'ensemble du processus d'identification de la cible et des pr√©dicteurs, la recherche de la relation, l'estimation des coefficients, la recherche des valeurs pr√©dites de la cible, et enfin l'√©valuation de la pr√©cision de la relation ajust√©e.\n","Avantages de l'utilisation de l'analyse de r√©gression La r√©gression explore les relations significatives entre la variable d√©pendante et la variable ind√©pendante Indique la force de l'impact de plusieurs variables ind√©pendantes sur une variable d√©pendante Permet de comparer l'effet des mesures de variables sur diff√©rentes √©chelles et peut consid√©rer des variables nominales, d'intervalle ou cat√©goriques pour l'analyse R√©gression lin√©aire : La r√©gression lin√©aire est l'une des techniques de mod√©lisation pr√©dictive les plus couramment utilis√©es. Elle est repr√©sent√©e par une √©quation ùëå = ùëé +\n","ùëèùëã + ùëí, o√π a est l'ordonn√©e √† l'origine, b est la pente de la ligne et e est le terme d'erreur. Cette √©quation peut √™tre utilis√©e pour pr√©dire la valeur d'une variable cible en fonction d'une ou plusieurs variables pr√©dicteurs donn√©es Logistique R√©gression: La r√©gression logistique est utilis√©e pour expliquer la relation entre une variable binaire d√©pendante et une ou plusieurs variables ind√©pendantes de niveau nominal, ordinal, intervalle ou ratio R√©gression polynomiale : Une √©quation de r√©gression est une √©quation de r√©gression polynomiale si la puissance de la variable ind√©pendante est sup√©rieure √† 1. L'√©quation ci-dessous repr√©sente une √©quation polynomiale. ùëå = ùëé + ùëèùëã + ùëêùëã2. Dans cette technique de r√©gression, la ligne de meilleur ajustement n'est pas une ligne droite. Il s'agit plut√¥t d'une courbe qui s'adapte aux points de donn√©es.\n","\"\"\"\n","corpus = data.lower().split(\".\")\n","\n","\n","tokenizer.fit_on_texts(corpus)\n","total_words = len(tokenizer.word_index) + 1\n","\n","# create input sequences using list of tokens\n","input_sequences = []\n","for line in corpus:\n","\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n","\tfor i in range(1, len(token_list)):\n","\t\tn_gram_sequence = token_list[:i+1]\n","\t\tinput_sequences.append(n_gram_sequence)\n","\n","\n","# pad sequences \n","max_sequence_len = max([len(x) for x in input_sequences])\n","input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n","\n","# create predictors and label\n","predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n","\n","label = ku.to_categorical(label, num_classes=total_words)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading...\n","From: https://drive.google.com/uc?id=108jAePKK4R3BVYBbYJZ32JWUwxeMg20K\n","To: /content/sonnets.txt\n","\r  0% 0.00/93.6k [00:00<?, ?B/s]\r100% 93.6k/93.6k [00:00<00:00, 37.8MB/s]\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VVDTqGb1bOxn","outputId":"5f58d295-99c5-4802-c6ab-68389d752e3a"},"source":["### \n","\n","model = Sequential()\n","model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))#  Embedding Layer\n","model.add(Bidirectional(LSTM(40, return_sequences = True)))# LSTM Layer)\n","model.add(Bidirectional(LSTM(20)))# LSTM Layer)\n","\n","\n","model.add(Dense(total_words, activation='relu'))\n","model.add(Dense(total_words, activation='softmax'))\n","# optimizer\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","### \n","\n","print(model.summary())"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 83, 100)           111400    \n","                                                                 \n"," bidirectional (Bidirectiona  (None, 83, 80)           45120     \n"," l)                                                              \n","                                                                 \n"," bidirectional_1 (Bidirectio  (None, 40)               16160     \n"," nal)                                                            \n","                                                                 \n"," dense (Dense)               (None, 1114)              45674     \n","                                                                 \n"," dense_1 (Dense)             (None, 1114)              1242110   \n","                                                                 \n","=================================================================\n","Total params: 1,460,464\n","Trainable params: 1,460,464\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bPrOFdabcYSv","outputId":"651d303b-0fe9-48b5-cb40-6a11a9e83eaa"},"source":["history = model.fit(predictors, label, epochs=150, verbose=1)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/150\n","135/135 [==============================] - 28s 151ms/step - loss: 6.3127 - accuracy: 0.0520\n","Epoch 2/150\n","135/135 [==============================] - 20s 150ms/step - loss: 5.8846 - accuracy: 0.0576\n","Epoch 3/150\n","135/135 [==============================] - 20s 151ms/step - loss: 5.6394 - accuracy: 0.0578\n","Epoch 4/150\n","135/135 [==============================] - 20s 149ms/step - loss: 5.3580 - accuracy: 0.0734\n","Epoch 5/150\n","135/135 [==============================] - 20s 149ms/step - loss: 5.0827 - accuracy: 0.0836\n","Epoch 6/150\n","135/135 [==============================] - 20s 151ms/step - loss: 4.8173 - accuracy: 0.0934\n","Epoch 7/150\n","135/135 [==============================] - 20s 150ms/step - loss: 4.5334 - accuracy: 0.1110\n","Epoch 8/150\n","135/135 [==============================] - 20s 149ms/step - loss: 4.2309 - accuracy: 0.1319\n","Epoch 9/150\n","135/135 [==============================] - 20s 150ms/step - loss: 3.9360 - accuracy: 0.1570\n","Epoch 10/150\n","135/135 [==============================] - 20s 151ms/step - loss: 3.6695 - accuracy: 0.1854\n","Epoch 11/150\n","135/135 [==============================] - 20s 150ms/step - loss: 3.4187 - accuracy: 0.2193\n","Epoch 12/150\n","135/135 [==============================] - 20s 151ms/step - loss: 3.1777 - accuracy: 0.2590\n","Epoch 13/150\n","135/135 [==============================] - 20s 150ms/step - loss: 2.9454 - accuracy: 0.2906\n","Epoch 14/150\n","135/135 [==============================] - 20s 150ms/step - loss: 2.7219 - accuracy: 0.3319\n","Epoch 15/150\n","135/135 [==============================] - 20s 150ms/step - loss: 2.5056 - accuracy: 0.3756\n","Epoch 16/150\n","135/135 [==============================] - 20s 150ms/step - loss: 2.3112 - accuracy: 0.4098\n","Epoch 17/150\n","135/135 [==============================] - 20s 150ms/step - loss: 2.1118 - accuracy: 0.4576\n","Epoch 18/150\n","135/135 [==============================] - 20s 151ms/step - loss: 1.9265 - accuracy: 0.4864\n","Epoch 19/150\n","135/135 [==============================] - 20s 151ms/step - loss: 1.7723 - accuracy: 0.5317\n","Epoch 20/150\n","135/135 [==============================] - 20s 149ms/step - loss: 1.6348 - accuracy: 0.5700\n","Epoch 21/150\n","135/135 [==============================] - 20s 151ms/step - loss: 1.4858 - accuracy: 0.6014\n","Epoch 22/150\n","135/135 [==============================] - 20s 150ms/step - loss: 1.3555 - accuracy: 0.6341\n","Epoch 23/150\n","135/135 [==============================] - 20s 150ms/step - loss: 1.2539 - accuracy: 0.6697\n","Epoch 24/150\n","135/135 [==============================] - 20s 150ms/step - loss: 1.1610 - accuracy: 0.6880\n","Epoch 25/150\n","135/135 [==============================] - 20s 150ms/step - loss: 1.0440 - accuracy: 0.7194\n","Epoch 26/150\n","135/135 [==============================] - 20s 148ms/step - loss: 0.9529 - accuracy: 0.7429\n","Epoch 27/150\n","135/135 [==============================] - 20s 149ms/step - loss: 0.8801 - accuracy: 0.7689\n","Epoch 28/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.8095 - accuracy: 0.7786\n","Epoch 29/150\n","135/135 [==============================] - 20s 151ms/step - loss: 0.7318 - accuracy: 0.8060\n","Epoch 30/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.6738 - accuracy: 0.8211\n","Epoch 31/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.6125 - accuracy: 0.8418\n","Epoch 32/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.5920 - accuracy: 0.8432\n","Epoch 33/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.5091 - accuracy: 0.8718\n","Epoch 34/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.5002 - accuracy: 0.8725\n","Epoch 35/150\n","135/135 [==============================] - 20s 151ms/step - loss: 0.4695 - accuracy: 0.8736\n","Epoch 36/150\n","135/135 [==============================] - 20s 152ms/step - loss: 0.4410 - accuracy: 0.8825\n","Epoch 37/150\n","135/135 [==============================] - 20s 151ms/step - loss: 0.3807 - accuracy: 0.9031\n","Epoch 38/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.3498 - accuracy: 0.9066\n","Epoch 39/150\n","135/135 [==============================] - 20s 151ms/step - loss: 0.3032 - accuracy: 0.9247\n","Epoch 40/150\n","135/135 [==============================] - 21s 152ms/step - loss: 0.2814 - accuracy: 0.9292\n","Epoch 41/150\n","135/135 [==============================] - 20s 151ms/step - loss: 0.2655 - accuracy: 0.9294\n","Epoch 42/150\n","135/135 [==============================] - 20s 151ms/step - loss: 0.2517 - accuracy: 0.9389\n","Epoch 43/150\n","135/135 [==============================] - 20s 151ms/step - loss: 0.2405 - accuracy: 0.9405\n","Epoch 44/150\n","135/135 [==============================] - 21s 152ms/step - loss: 0.2495 - accuracy: 0.9375\n","Epoch 45/150\n","135/135 [==============================] - 20s 151ms/step - loss: 0.2357 - accuracy: 0.9440\n","Epoch 46/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.2161 - accuracy: 0.9447\n","Epoch 47/150\n","135/135 [==============================] - 20s 151ms/step - loss: 0.2834 - accuracy: 0.9233\n","Epoch 48/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.2869 - accuracy: 0.9220\n","Epoch 49/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.2560 - accuracy: 0.9324\n","Epoch 50/150\n","135/135 [==============================] - 20s 149ms/step - loss: 0.2574 - accuracy: 0.9305\n","Epoch 51/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.2144 - accuracy: 0.9443\n","Epoch 52/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.1578 - accuracy: 0.9584\n","Epoch 53/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.1566 - accuracy: 0.9587\n","Epoch 54/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.1457 - accuracy: 0.9614\n","Epoch 55/150\n","135/135 [==============================] - 20s 149ms/step - loss: 0.1285 - accuracy: 0.9645\n","Epoch 56/150\n","135/135 [==============================] - 20s 149ms/step - loss: 0.1204 - accuracy: 0.9677\n","Epoch 57/150\n","135/135 [==============================] - 20s 148ms/step - loss: 0.1167 - accuracy: 0.9668\n","Epoch 58/150\n","135/135 [==============================] - 20s 148ms/step - loss: 0.1130 - accuracy: 0.9679\n","Epoch 59/150\n","135/135 [==============================] - 20s 149ms/step - loss: 0.1145 - accuracy: 0.9672\n","Epoch 60/150\n","135/135 [==============================] - 20s 149ms/step - loss: 0.1167 - accuracy: 0.9668\n","Epoch 61/150\n","135/135 [==============================] - 20s 149ms/step - loss: 0.1146 - accuracy: 0.9670\n","Epoch 62/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.1803 - accuracy: 0.9535\n","Epoch 63/150\n","135/135 [==============================] - 20s 149ms/step - loss: 0.5977 - accuracy: 0.8239\n","Epoch 64/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.4785 - accuracy: 0.8504\n","Epoch 65/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.2583 - accuracy: 0.9238\n","Epoch 66/150\n","135/135 [==============================] - 20s 148ms/step - loss: 0.1383 - accuracy: 0.9614\n","Epoch 67/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.1091 - accuracy: 0.9679\n","Epoch 68/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.1003 - accuracy: 0.9689\n","Epoch 69/150\n","135/135 [==============================] - 20s 149ms/step - loss: 0.0994 - accuracy: 0.9682\n","Epoch 70/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.0958 - accuracy: 0.9691\n","Epoch 71/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.0936 - accuracy: 0.9703\n","Epoch 72/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.0954 - accuracy: 0.9689\n","Epoch 73/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.0955 - accuracy: 0.9696\n","Epoch 74/150\n","135/135 [==============================] - 20s 149ms/step - loss: 0.0944 - accuracy: 0.9698\n","Epoch 75/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.0925 - accuracy: 0.9684\n","Epoch 76/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.0957 - accuracy: 0.9686\n","Epoch 77/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.0999 - accuracy: 0.9689\n","Epoch 78/150\n","135/135 [==============================] - 20s 149ms/step - loss: 0.1182 - accuracy: 0.9645\n","Epoch 79/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.1513 - accuracy: 0.9531\n","Epoch 80/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.2563 - accuracy: 0.9226\n","Epoch 81/150\n","135/135 [==============================] - 20s 149ms/step - loss: 0.3449 - accuracy: 0.8906\n","Epoch 82/150\n","135/135 [==============================] - 20s 149ms/step - loss: 0.2458 - accuracy: 0.9261\n","Epoch 83/150\n","135/135 [==============================] - 20s 149ms/step - loss: 0.1435 - accuracy: 0.9584\n","Epoch 84/150\n","135/135 [==============================] - 20s 148ms/step - loss: 0.1098 - accuracy: 0.9631\n","Epoch 85/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.0978 - accuracy: 0.9677\n","Epoch 86/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.0912 - accuracy: 0.9686\n","Epoch 87/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.0871 - accuracy: 0.9707\n","Epoch 88/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.0901 - accuracy: 0.9668\n","Epoch 89/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.0844 - accuracy: 0.9705\n","Epoch 90/150\n","135/135 [==============================] - 20s 148ms/step - loss: 0.0846 - accuracy: 0.9710\n","Epoch 91/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.0858 - accuracy: 0.9710\n","Epoch 92/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.0816 - accuracy: 0.9710\n","Epoch 93/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.0844 - accuracy: 0.9689\n","Epoch 94/150\n","135/135 [==============================] - 20s 151ms/step - loss: 0.0840 - accuracy: 0.9703\n","Epoch 95/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.0842 - accuracy: 0.9693\n","Epoch 96/150\n","135/135 [==============================] - 20s 148ms/step - loss: 0.0845 - accuracy: 0.9693\n","Epoch 97/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.0949 - accuracy: 0.9679\n","Epoch 98/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.1028 - accuracy: 0.9642\n","Epoch 99/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.1994 - accuracy: 0.9408\n","Epoch 100/150\n","135/135 [==============================] - 20s 151ms/step - loss: 0.4453 - accuracy: 0.8657\n","Epoch 101/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.2753 - accuracy: 0.9110\n","Epoch 102/150\n","135/135 [==============================] - 20s 148ms/step - loss: 0.1634 - accuracy: 0.9496\n","Epoch 103/150\n","135/135 [==============================] - 20s 149ms/step - loss: 0.1050 - accuracy: 0.9661\n","Epoch 104/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.0840 - accuracy: 0.9682\n","Epoch 105/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.0801 - accuracy: 0.9693\n","Epoch 106/150\n","135/135 [==============================] - 20s 149ms/step - loss: 0.0798 - accuracy: 0.9710\n","Epoch 107/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.0791 - accuracy: 0.9703\n","Epoch 108/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.0765 - accuracy: 0.9700\n","Epoch 109/150\n","135/135 [==============================] - 20s 151ms/step - loss: 0.0792 - accuracy: 0.9703\n","Epoch 110/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.0785 - accuracy: 0.9710\n","Epoch 111/150\n","135/135 [==============================] - 20s 148ms/step - loss: 0.0793 - accuracy: 0.9691\n","Epoch 112/150\n","135/135 [==============================] - 20s 151ms/step - loss: 0.0780 - accuracy: 0.9717\n","Epoch 113/150\n","135/135 [==============================] - 20s 151ms/step - loss: 0.0784 - accuracy: 0.9696\n","Epoch 114/150\n","135/135 [==============================] - 20s 149ms/step - loss: 0.0784 - accuracy: 0.9705\n","Epoch 115/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.0776 - accuracy: 0.9703\n","Epoch 116/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.0779 - accuracy: 0.9693\n","Epoch 117/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.0781 - accuracy: 0.9710\n","Epoch 118/150\n","135/135 [==============================] - 20s 148ms/step - loss: 0.0768 - accuracy: 0.9691\n","Epoch 119/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.0770 - accuracy: 0.9717\n","Epoch 120/150\n","135/135 [==============================] - 20s 148ms/step - loss: 0.0768 - accuracy: 0.9698\n","Epoch 121/150\n","135/135 [==============================] - 20s 149ms/step - loss: 0.0765 - accuracy: 0.9700\n","Epoch 122/150\n","135/135 [==============================] - 20s 147ms/step - loss: 0.0764 - accuracy: 0.9698\n","Epoch 123/150\n","135/135 [==============================] - 20s 149ms/step - loss: 0.2614 - accuracy: 0.9206\n","Epoch 124/150\n","135/135 [==============================] - 20s 149ms/step - loss: 0.5180 - accuracy: 0.8462\n","Epoch 125/150\n","135/135 [==============================] - 20s 149ms/step - loss: 0.2326 - accuracy: 0.9247\n","Epoch 126/150\n","135/135 [==============================] - 20s 147ms/step - loss: 0.1184 - accuracy: 0.9589\n","Epoch 127/150\n","135/135 [==============================] - 20s 149ms/step - loss: 0.0881 - accuracy: 0.9689\n","Epoch 128/150\n","135/135 [==============================] - 20s 149ms/step - loss: 0.0775 - accuracy: 0.9714\n","Epoch 129/150\n","135/135 [==============================] - 20s 149ms/step - loss: 0.0734 - accuracy: 0.9707\n","Epoch 130/150\n","135/135 [==============================] - 20s 148ms/step - loss: 0.0752 - accuracy: 0.9703\n","Epoch 131/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.0736 - accuracy: 0.9714\n","Epoch 132/150\n","135/135 [==============================] - 20s 149ms/step - loss: 0.0720 - accuracy: 0.9714\n","Epoch 133/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.0741 - accuracy: 0.9705\n","Epoch 134/150\n","135/135 [==============================] - 20s 149ms/step - loss: 0.0727 - accuracy: 0.9717\n","Epoch 135/150\n","135/135 [==============================] - 20s 148ms/step - loss: 0.0728 - accuracy: 0.9719\n","Epoch 136/150\n","135/135 [==============================] - 20s 149ms/step - loss: 0.0728 - accuracy: 0.9712\n","Epoch 137/150\n","135/135 [==============================] - 20s 151ms/step - loss: 0.0750 - accuracy: 0.9703\n","Epoch 138/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.0731 - accuracy: 0.9698\n","Epoch 139/150\n","135/135 [==============================] - 20s 151ms/step - loss: 0.0733 - accuracy: 0.9712\n","Epoch 140/150\n","135/135 [==============================] - 21s 152ms/step - loss: 0.0742 - accuracy: 0.9710\n","Epoch 141/150\n","135/135 [==============================] - 20s 152ms/step - loss: 0.0715 - accuracy: 0.9710\n","Epoch 142/150\n","135/135 [==============================] - 20s 151ms/step - loss: 0.0721 - accuracy: 0.9696\n","Epoch 143/150\n","135/135 [==============================] - 20s 151ms/step - loss: 0.0709 - accuracy: 0.9710\n","Epoch 144/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.0728 - accuracy: 0.9719\n","Epoch 145/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.0747 - accuracy: 0.9707\n","Epoch 146/150\n","135/135 [==============================] - 20s 151ms/step - loss: 0.0714 - accuracy: 0.9712\n","Epoch 147/150\n","135/135 [==============================] - 20s 151ms/step - loss: 0.0724 - accuracy: 0.9717\n","Epoch 148/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.0722 - accuracy: 0.9700\n","Epoch 149/150\n","135/135 [==============================] - 20s 151ms/step - loss: 0.0710 - accuracy: 0.9719\n","Epoch 150/150\n","135/135 [==============================] - 20s 151ms/step - loss: 0.0728 - accuracy: 0.9712\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UoPHfe32cbB-","outputId":"6ca02c55-b027-4868-ae20-36be96f6acb3"},"source":["seed_text = \"Le syst√®me\"\n","next_words = 30\n","\n","for _ in range(next_words):\n","  token_list = tokenizer.texts_to_sequences([seed_text])[0]\n","  token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n","  predicted = model.predict(token_list, verbose=0)\n","  predicted_class = np.argmax(predicted)\n","  output_word = \"\"\n","  for word, index in tokenizer.word_index.items():\n","    if index == predicted_class:\n","      output_word = word\n","      #break\n","      seed_text += \" \" + output_word\n","  print(seed_text)\n","\n"," "],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Le syst√®me automatis√©\n","Le syst√®me automatis√© qui\n","Le syst√®me automatis√© qui est\n","Le syst√®me automatis√© qui est fourni\n","Le syst√®me automatis√© qui est fourni par\n","Le syst√®me automatis√© qui est fourni par l'adas\n","Le syst√®me automatis√© qui est fourni par l'adas au\n","Le syst√®me automatis√© qui est fourni par l'adas au v√©hicule\n","Le syst√®me automatis√© qui est fourni par l'adas au v√©hicule a\n","Le syst√®me automatis√© qui est fourni par l'adas au v√©hicule a prouv√©\n","Le syst√®me automatis√© qui est fourni par l'adas au v√©hicule a prouv√© qu'il\n","Le syst√®me automatis√© qui est fourni par l'adas au v√©hicule a prouv√© qu'il permettait\n","Le syst√®me automatis√© qui est fourni par l'adas au v√©hicule a prouv√© qu'il permettait de\n","Le syst√®me automatis√© qui est fourni par l'adas au v√©hicule a prouv√© qu'il permettait de r√©duire\n","Le syst√®me automatis√© qui est fourni par l'adas au v√©hicule a prouv√© qu'il permettait de r√©duire le\n","Le syst√®me automatis√© qui est fourni par l'adas au v√©hicule a prouv√© qu'il permettait de r√©duire le nombre\n","Le syst√®me automatis√© qui est fourni par l'adas au v√©hicule a prouv√© qu'il permettait de r√©duire le nombre de\n","Le syst√®me automatis√© qui est fourni par l'adas au v√©hicule a prouv√© qu'il permettait de r√©duire le nombre de d√©c√®s\n","Le syst√®me automatis√© qui est fourni par l'adas au v√©hicule a prouv√© qu'il permettait de r√©duire le nombre de d√©c√®s et\n","Le syst√®me automatis√© qui est fourni par l'adas au v√©hicule a prouv√© qu'il permettait de r√©duire le nombre de d√©c√®s et des\n","Le syst√®me automatis√© qui est fourni par l'adas au v√©hicule a prouv√© qu'il permettait de r√©duire le nombre de d√©c√®s et des d√©g√¢ts\n","Le syst√®me automatis√© qui est fourni par l'adas au v√©hicule a prouv√© qu'il permettait de r√©duire le nombre de d√©c√®s et des d√©g√¢ts d‚Äôune\n","Le syst√®me automatis√© qui est fourni par l'adas au v√©hicule a prouv√© qu'il permettait de r√©duire le nombre de d√©c√®s et des d√©g√¢ts d‚Äôune mani√®re\n","Le syst√®me automatis√© qui est fourni par l'adas au v√©hicule a prouv√© qu'il permettait de r√©duire le nombre de d√©c√®s et des d√©g√¢ts d‚Äôune mani√®re remarquable\n","Le syst√®me automatis√© qui est fourni par l'adas au v√©hicule a prouv√© qu'il permettait de r√©duire le nombre de d√©c√®s et des d√©g√¢ts d‚Äôune mani√®re remarquable sur\n","Le syst√®me automatis√© qui est fourni par l'adas au v√©hicule a prouv√© qu'il permettait de r√©duire le nombre de d√©c√®s et des d√©g√¢ts d‚Äôune mani√®re remarquable sur les\n","Le syst√®me automatis√© qui est fourni par l'adas au v√©hicule a prouv√© qu'il permettait de r√©duire le nombre de d√©c√®s et des d√©g√¢ts d‚Äôune mani√®re remarquable sur les routes\n","Le syst√®me automatis√© qui est fourni par l'adas au v√©hicule a prouv√© qu'il permettait de r√©duire le nombre de d√©c√®s et des d√©g√¢ts d‚Äôune mani√®re remarquable sur les routes en\n","Le syst√®me automatis√© qui est fourni par l'adas au v√©hicule a prouv√© qu'il permettait de r√©duire le nombre de d√©c√®s et des d√©g√¢ts d‚Äôune mani√®re remarquable sur les routes en minimisant\n","Le syst√®me automatis√© qui est fourni par l'adas au v√©hicule a prouv√© qu'il permettait de r√©duire le nombre de d√©c√®s et des d√©g√¢ts d‚Äôune mani√®re remarquable sur les routes en minimisant les\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xboR_C-1lrQL","outputId":"17ab5558-5359-4522-98c3-9c3fe3019e92"},"source":["print(\"\\nle texte final:\", seed_text)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","le texte final: Le syst√®me automatis√© qui est fourni par l'adas au v√©hicule a prouv√© qu'il permettait de r√©duire le nombre de d√©c√®s et des d√©g√¢ts d‚Äôune mani√®re remarquable sur les routes en minimisant les\n"]}]}]}
