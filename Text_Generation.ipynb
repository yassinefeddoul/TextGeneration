{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TextGeneration.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"ZTI8UsQ8afxe"},"source":["from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras import regularizers\n","import tensorflow.keras.utils as ku \n","import numpy as np "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0-G--MnRblJX"},"source":["from google.colab import drive"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tWoUzsifbmJ-","outputId":"a16cdada-9155-4307-d345-d6da6fb21080"},"source":["drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","metadata":{"id":"NQ_sTrd-boY-"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5OpRcIq2aglg","outputId":"3bbd48ed-2307-4598-d562-bd5895953e0c"},"source":["tokenizer = Tokenizer()\n","\n","# sonnets.txt\n","!gdown --id 108jAePKK4R3BVYBbYJZ32JWUwxeMg20K\n","\n","#data = open('./Projet_Metier.txt').read()\n","data = \"\"\"\n","La plupart des accidents de la route sont dus à une erreur humaine, les systèmes avancés d'aide à la conduite (ADAS) sont des systèmes développés pour automatiser, adapter et améliorer le véhicule pour plus de sécurité et une meilleure conduite.\n","Le système automatisé qui est fourni par l'ADAS au véhicule a prouvé qu'il permettait de réduire le nombre de décès et des dégâts d’une manière remarquable sur les routes, en minimisant les erreurs humaines.\n","Les dispositifs de sécurité sont conçus pour éviter les collisions et les accidents en proposant des technologies qui alertent le conducteur en cas de problème potentiel ou pour éviter les collisions en mettant en place des mesures de protection et en prenant le contrôle du véhicule.\n","Nous souhaitons déployer un système d’assistance à la conduite ADAS (Advanced Driving Assistance System) qui s'appuie sur des technologies de l'intelligence artificielle.\n","Pour subvenir à ces besoins, il est primordial de passer par la prédiction des trajectoires des objets mobiles.\n","Dans la première partie du projet nous avons vu de manière globale les systèmes d’aide à la conduite pour les véhicules autonomes, nous avons vu que ces véhicules nécessitent l’interactions avec leur entourage, que ce soit à la suite d'un lent véhicule, se coordonnant pour se relayer avec les véhicules aux intersections, ou manœuvrer autour d'autres véhicules pour atteindre le stationnement spots, il est presque impossible de faire un voyage en voiture sans être affecté par un autre véhicule d'une manière ou d'une autre.\n","Les systèmes d'assistance et les véhicules autonomes deviennent plus sophistiqués, le raisonnement sur ces interactions avec les véhicules devient de plus en plus important.\n","Notre rôle consiste à manifeste à prédire les trajectoires des objets mobiles, en se basant sur l’ensemble d’information collectées a l’intermédiaire des capteurs, cameras, qui vont être traiter à la suite pour aider à déterminer l’ensembles de décisions futures que l’automobile va prendre. \n","Nous savons probablement tous que les ordinateurs et les algorithmes s'améliorent chaque jour pour \"penser\", analyser des situations et prendre des décisions comme le font les humains. La compréhension de la vision par ordinateur fait partie intégrante de ces progrès dans le domaine de l'intelligence artificielle. Il y a 10 ans, la détection des objets était un mystère pour nous, il n'était même pas utile de mentionner le suivi des objets. Ces technologies ont fait de grands progrès à cet égard et les limites sont remises en question et repoussées au fur et à mesure que nous parlons.\n","Le système d’aide à la conduite ou système avancé d’aide à la conduite est un système de sécurité active d'information ou d'assistance du conducteur pour éviter l'apparition d'une situation dangereuse risquant d'aboutir à un accident.\n","Si la détection d'objets dans une image a fait l'objet d'une grande attention de la part de la communauté scientifique, un domaine moins connu et pourtant aux applications très répandues est le suivi d'objets dans une vidéo ou un flux vidéo en temps réel. Il s'agit d'un domaine qui nécessite de fusionner nos connaissances en matière de détection d'objets dans les images avec l'analyse des informations temporelles et leur utilisation pour prédire les trajectoires des mouvements.\n","Le suivi d'objets peut être utilisé dans le cadre d'événements sportifs, par exemple pour le suivi d'un match de basket-ball, pour attraper des cambrioleurs, pour compter les voitures qui passent dans la rue ou même pour repérer le moment où votre chien court à l'extérieur de la maison. Il existe de nombreux domaines dans lesquels nous pouvons utiliser le suivi d'objets.\n","Les systèmes d'assistance à la conduite augmentent la sécurité et la rentabilité des voitures, des utilitaires et des véhicules spéciaux et sont des éléments clés de la mobilité de demain. Ils incluent des applications modernes telles que l'aide à la marche arrière, le régulateur de vitesse adaptatif (ACC), la reconnaissance des panneaux de signalisation, les assistants de changement de direction, d’angle mort et de maintien de trajectoire ou encore la vue panoramique à 360°, lesquelles ne sauraient se passer de composants tels que des caméras robustes, des capteurs optiques et des systèmes LIDAR. First Sensor fournit à des équipementiers de première monte, des intégrateurs et des équipementiers de deuxième monte opérant à différents niveaux de la chaîne de création de valeur des produits allant du composant au module et au système complet.\n","L'object tracker, quant à lui, doit suivre un objet particulier sur toute la série d'images (par exemple une vidéo). Si le détecteur détecte deux bananes dans l'image, le tracker d'objets doit identifier les deux détections distinctes et doit les suivre dans les images suivantes (à l'aide d'un identifiant d'objet unique).\n","L'occlusion d'objets dans les vidéos est l'un des obstacles les plus courants au suivi des objets. Comme vous pouvez le voir, dans la figure ci-dessus (à gauche), l'homme à l'arrière- plan est détecté, alors que le même homme n'est pas détecté dans l'image suivante (à droite). Maintenant, le défi pour le traqueur consiste à identifier le même homme lorsqu'il est détecté dans une image beaucoup plus tard et à associer son ancienne trace et ses caractéristiques à sa trajectoire.\n","Notre projet a comme objectif de détecter et suivre les objets mobiles afin de prédire leurs comportements futurs.\n","En résumé ce rapport se compose de deux chapitres, le premier résume la bibliographie nécessaire pour la compréhension préliminaire du sujet et pouvoir entamer notre \n","sujet, le deuxième concerne la partie pratique de notre projet où on a implémenté les différentes solutions résolvant notre problématique, faire une étude comparative ainsi conclure la solution la plus adéquate pour notre projet.\n","Comme nous sommes dans la première phase d'un projet de grande envergure, ce rapport fournit également une vaste bibliographie pour le sujet discuté.\n","La plupart des accidents de la route sont dus à une erreur humaine, les systèmes avancés d'aide à la conduite (ADAS) sont des systèmes développés pour automatiser, adapter et améliorer le véhicule pour plus de sécurité et une meilleure conduite. \n","Le système automatisé qui est fourni par l'ADAS au véhicule a prouvé qu'il permettait de réduire le nombre de décès et des dégâts d’une manière remarquable sur les routes, en minimisant les erreurs humaines. Les dispositifs de sécurité sont conçus pour éviter les collisions et les accidents en proposant des technologies qui alertent le conducteur en cas de \n","problème potentiel ou pour éviter les collisions en mettant en place des mesures de protection \n","et en prenant le contrôle du véhicule.\n","Nous souhaitons déployer un système d’assistance à la conduite ADAS (Advanced Driving Assistance System) qui s'appuie sur des technologies de l'intelligence artificielle. \n","Pour subvenir à ces besoins, il est primordial de passer par la prédiction des trajectoires des objets mobiles\n","La conduite dans un environnement urbain nécessite de l’interaction avec d’autres véhicules, que ce soit à la suite d'un lent véhicule, se coordonnant pour se relayer avec les véhicules aux intersections, ou manœuvrer autour d'autres véhicules pour atteindre le stationnement spots, il est presque impossible de faire un voyage en voiture sans être affecté par un autre véhicule d'une manière ou d'une autre.\n","En tant que conducteur, les systèmes d'assistance et les véhicules autonomes \n","deviennent plus sophistiqué, le raisonnement sur ces interactions avec les véhicules \n","deviennent de plus en plus importants. Dont notre rôle commence, qui se manifeste à prédire \n","les trajectoires des objets mobiles, en se basant sur l’ensemble d’information collectées a \n","l’intermédiaire des capteurs, cameras ..., qui vont être traiter à la suite pour aider à déterminer \n","l’ensembles de décisions futur que l’automobile va prendre.\n","Les systèmes avancés d'aide à la conduite sont des systèmes intelligents qui résident à \n","l'intérieur du véhicule et assistent le conducteur principal de diverses manières. Ces systèmes \n","peuvent être utilisés pour fournir des informations vitales sur le trafic, la fermeture et le \n","blocage des routes, les niveaux d'embouteillage, les itinéraires suggérés pour éviter les \n","embouteillages, etc.\n","Pour donner une idée de ce que les systèmes d'aide à la conduite représentent pour les \n","utilisateurs, nous présentons un aperçu des technologies existantes. Pour des raisons de \n","commodité, elles ont été divisées en sous-catégories. Ce bref aperçu de la technologie ADAS \n","existante ne met en évidence que les types d'ADAS les plus \"courants\".\n","La plupart des accidents de la route sont dus à une erreur humaine, les systèmes avancés d'aide à la conduite (ADAS) sont des systèmes développés pour automatiser, adapter et améliorer le véhicule pour plus de sécurité et une meilleure conduite.\n","Le système automatisé qui est fourni par l'ADAS au véhicule a prouvé qu'il permettait de réduire le nombre de décès et des dégâts d’une manière remarquable sur les routes, en minimisant les erreurs humaines. Les dispositifs de sécurité sont conçus pour éviter les collisions et les accidents en proposant des technologies qui alertent le conducteur en cas de problème potentiel ou pour éviter les collisions en mettant en place des mesures de protection et en prenant le contrôle du véhicule.\n","Nous souhaitons déployer un système d’assistance à la conduite ADAS (Advanced Driving Assistance System) qui s'appuie sur des technologies de l'intelligence artificielle. Pour subvenir à ces besoins, il est primordial de passer par la prédiction des trajectoires des objets mobiles.\n","Pour cadrer notre travaille nous avons élaborer un cahier de charge qui se compose de 4 axes principales\n","contexte et présentation du projet, dont on a défini de façon brève notre projet,\n","analyse du besoin, où on a exprimé l’utilité de notre projet,\n","Expression fonctionnelle du besoin où les contraintes et les fonctionnalités de notre système sont décrites,\n","Planning de réalisation.\n","Pour aboutir à cet objectif, nous devons Tracker les objets mobiles et Prédire les trajectoires des objets mobiles.\n","Dans la première partie du projet nous avons vu de manière globale les systèmes d’aide à la conduite pour les véhicules autonomes, nous avons vu que ces véhicules nécessitent l’interactions avec leur entourage, que ce soit à la suite d'un lent véhicule, se coordonnant pour se relayer avec les véhicules aux intersections, ou manœuvrer autour d'autres véhicules pour atteindre le stationnement spots, il est presque impossible de faire un voyage en voiture sans être affecté par un autre véhicule d'une manière ou d'une autre.\n","Les systèmes d'assistance et les véhicules autonomes deviennent plus sophistiqués, le raisonnement sur ces interactions avec les véhicules devient de plus en plus importants.\n","Notre rôle consiste à manifeste à prédire les trajectoires des objets mobiles, en se basant sur l’ensemble d’information collectées a l’intermédiaire des capteurs, cameras ..., qui vont être traiter à la suite pour aider à déterminer l’ensembles de décisions futur que l’automobile va prendre. \n","Nous savons probablement tous que les ordinateurs et les algorithmes s'améliorent chaque jour pour \"penser\", analyser des situations et prendre des décisions comme le font les humains. La compréhension de la vision par ordinateur fait partie intégrante de ces progrès dans le domaine de l'intelligence artificielle. Il y a 10 ans, la détection des objets était un mystère pour nous, il n'était même pas utile de mentionner le suivi des objets. Ces technologies ont fait de grands progrès à cet égard et les limites sont remises en question et repoussées au fur et à mesure que nous parlons.\n","Si la détection d'objets dans une image a fait l'objet d'une grande attention de la part de la communauté scientifique, un domaine moins connu et pourtant aux applications très répandues est le suivi d'objets dans une vidéo ou un flux vidéo en temps réel. Il s'agit d'un domaine qui nécessite de fusionner nos connaissances en matière de détection d'objets dans les images avec l'analyse des informations temporelles et leur utilisation pour prédire les trajectoires des mouvements.\n","Le suivi d'objets peut être utilisé dans le cadre d'événements sportifs, par exemple pour le suivi d'un match de basket-ball, pour attraper des cambrioleurs, pour compter les voitures qui passent dans la rue ou même pour repérer le moment où votre chien court à l'extérieur de la maison. Il existe de nombreux domaines dans lesquels nous pouvons utiliser le suivi d'objets.\n","En termes simples, dans la détection d'objets, nous détectons un objet dans une image, nous l'entourons d'un bounding box ou d'un masque et nous le classons. Notez que le travail du détecteur s'arrête là. Il traite chaque image indépendamment et identifie de nombreux objets dans cette image particulière.\n","L'object tracker, quant à lui, doit suivre un objet particulier sur toute la série d'images (par exemple une vidéo). Si le détecteur détecte deux bananes dans l'image, le tracker d'objets doit identifier les deux détections distinctes et doit les suivre dans les images suivantes (à l'aide d'un identifiant d'objet unique).\n","L'occlusion d'objets dans les vidéos est l'un des obstacles les plus courants au suivi des objets. Comme vous pouvez le voir, dans la figure ci-dessus (à gauche), l'homme à l'arrière- plan est détecté, alors que le même homme n'est pas détecté dans l'image suivante (à droite). Maintenant, le défi pour le traqueur consiste à identifier le même homme lorsqu'il est détecté dans une image beaucoup plus tard et à associer son ancienne trace et ses caractéristiques à sa trajectoire.\n","Dans des exemples réels, la tâche consisterait à suivre un objet à travers différentes caméras, ce qui peut être utilisé dans des magasins d'IA où il n'y a pas de caissier et où nous devons suivre un client dans tout le magasin. Un exemple similaire est illustré ci-dessus. En conséquence, il y aura des changements significatifs dans la façon dont nous voyons l'objet\n","dans chaque caméra. Dans de tels cas, les caractéristiques utilisées pour suivre un objet deviennent très importantes car nous devons nous assurer qu'elles sont invariantes aux changements de vues.\n","Lorsque la caméra utilisée pour le suivi d'un objet particulier est également en mouvement pour cet objet, cela peut souvent entraîner des conséquences inattendues. Comme je l'ai mentionné précédemment, de nombreux trackers prennent en compte les caractéristiques d'un objet pour le suivre. Un tel tracker peut échouer dans des scénarios où l'objet apparaît différent à cause du mouvement de la caméra (il apparaît plus grand ou plus petit). Un tracker robuste pour ce problème peut être très utile dans des applications importantes comme les drones de suivi d'objets ou la navigation autonome.\n","L'une des choses les plus ennuyeuses dans la construction d'un tracker d'objets est d'obtenir de bonnes données d'entraînement pour un scénario particulier. Contrairement à la construction d'un ensemble de données pour un détecteur d'objets, nous pouvons avoir des images non connectées de manière aléatoire. Dans le suivi d'objets, il est nécessaire de disposer de séquences vidéo ou d'images où chaque instance de l'objet est identifiée de manière unique pour chaque image.\n","Mean-shift or Mode seeking est un algorithme populaire, principalement utilisé dans le clustering et d'autres problèmes non supervisés connexes. Il est similaire à K-Means mais remplace la technique simple du centroïd pour calculer les centres des clusters par une moyenne pondérée qui donne de l'importance aux points qui sont plus proches de la moyenne. L'objectif de l'algorithme est de trouver tous les modes dans la distribution de\n","données donnée. De plus, cet algorithme ne requiert pas une valeur \"K\" optimale comme K-Means\n","Supposons que nous ayons la détection d'un objet dans l'image et que nous extrayons certaines caractéristiques de la détection (couleur, texture, histogramme, etc.). En appliquant l'algorithme de Mean-shift, nous avons une idée générale de l'endroit où se trouve le mode de la distribution des caractéristiques dans l'état actuel. Maintenant, lorsque nous avons l'image suivante, où cette distribution a changé en raison du mouvement de l'objet dans l'image, l'algorithme de Mean-shift recherche le nouveau mode le plus important et suit donc l'objet.\n","Cette méthode diffère du mean-shift, car nous n'utilisons pas nécessairement les caractéristiques extraites de l'objet détecté. Au lieu de cela, l'objet est suivi en utilisant les variations spatio-temporelles de la luminosité de l'image au niveau des pixels.\n","Nous nous concentrons ici sur l'obtention d'un vecteur de déplacement pour l'objet à suivre à travers les images.\n","Dans presque tous les problèmes d'ingénierie qui impliquent une prédiction dans un sens temporel ou de série temporelle, qu'il s'agisse de vision par ordinateur, de guidage, de navigation, de systèmes de stabilisation ou même d'économie, vous entendrez souvent parler de \"filtre de Kalman\".\n","L'idée centrale d'un filtre de Kalman est d'utiliser les détections disponibles et les prédictions précédentes pour arriver à la meilleure estimation de l'état actuel tout en gardant la possibilité d'erreurs dans le processus.\n","Comme nous le voyons dans l'image ci-dessus, le filtre de Kalman fonctionne de manière récursive, où nous prenons les mesures actuelles, pour prédire l'état actuel, puis nous utilisons les mesures et mettons à jour nos prédictions. Donc, cela revient essentiellement à déduire une nouvelle distribution (les prédictions) à partir de la distribution de l'état précédent et de la distribution des mesures.\n","Par exemple, nous pouvons maintenant entraîner un assez bon détecteur d'objets YOLOv3 qui détecte une personne. Mais il n'est pas très précis et rate occasionnellement des détections, disons 10% des images. Pour suivre et prédire efficacement le prochain état d'une personne, supposons un \"modèle de vitesse constante\". Maintenant, une fois que nous avons défini le modèle simple selon les lois de la physique, étant donné une détection actuelle, nous\n","pouvons faire une bonne estimation de l'endroit où la personne sera dans la prochaine image. Tout cela semble parfait, dans un monde idéal, mais il y a toujours une composante de bruit.\n","L'une des premières méthodes qui a utilisé l'apprentissage profond, pour le suivi d'un seul objet, était les réseaux de régression profonde. Un modèle est entraîné sur un dataset composé de vidéos avec des images cibles étiquetées. L'objectif du modèle est de simplement suivre un objet donné à partir du cadre de l'image donnée.\n","Pour y parvenir, ils utilisent une architecture CNN à deux frames qui utilise à la fois le frame actuel et le frame précédente pour régresser avec précision sur l'objet.\n","Comme le montre la figure ci-dessus, nous prenons le cadre de l'image précédente en\n","fonction des prédictions et définissons une \"région de recherche\" dans l'image actuelle en fonction de ce cadre. Le réseau est ensuite entraîné à régresser pour trouver l'objet dans cette région de recherche. L'architecture du réseau est simple avec des CNN suivis de Fully connected layers qui nous donnent directement les coordonnées du Bounding Box. \n","ROLO \"Recurrent Yolo\" est une méthode élégante de suivre des objets en utilisant le Deep CNN. C'est un détecteur YOLO légèrement modifié avec une unité LSTM récurrente attachée à la fin, qui aide à suivre les objets en capturant les caractéristiques spatio-temporelles.\n","Comme le montre l'image ci-dessus, l'architecture est assez simple. Les détections de YOLO (bounding boxes) sont concaténées avec le vecteur de caractéristiques d'un extracteur de caractéristiques basé sur CNN (nous pouvons soit réutiliser le backend de YOLO, soit utiliser un extracteur de caractéristiques spécialisé). Maintenant, ce vecteur de caractéristiques concaténé, qui représente la plupart des informations spatiales liées à l'objet actuel, ainsi que les informations sur l'état précédent, est transmis à la cellule LSTM.\n","La sortie de la cellule tient maintenant compte des informations spatiales et temporelles. Cette simple astuce consistant à utiliser des CNN pour l'extraction de caractéristiques et des LSTM pour les prédictions de la boîte englobante a permis d'améliorer considérablement les problèmes de suivi. L'inconvénient de cette méthode est qu'il est assez difficile d'entraîner le réseau LSTM. \n","Le framework de suivi d'objets le plus populaire et l'un des plus utilisés est le Deep SORT (Simple Real-time Tracker).\n","Dans le tracker Deep Sort, le filtre de Kalman est un composant crucial. Le scénario de suivi de Kalman est défini sur l'espace d'état à huit dimensions (u, v, a, h, u', v', a', h') qui contient la position du centre de la boîte englobante (u, v), sont les centres des boîtes englobantes, a est le rapport d'aspect et h, la hauteur de l'image. Les autres variables sont les vitesses respectives des variables.\n","Notez que les variables n'ont que des facteurs de position et de vitesse absolus puisque nous supposons un modèle de vitesse linéaire simple. Le filtre de Kalman nous aide à prendre en compte le bruit dans la détection et utilise l'état antérieur pour prédire un bon ajustement des boîtes englobantes.\n","Pour chaque détection, nous créons une \"piste\", qui contient toutes les informations d'état nécessaires. Elle possède également un paramètre permettant de suivre et de supprimer les pistes dont la dernière détection réussie remonte à longtemps, car ces objets auraient quitté la scène. Pour éliminer les pistes en double, il existe un seuil de nombre minimum de détections pour les premières images.\n","Nous allons commencer par comprendre les SVM en termes simples. \n","La ligne sépare assez bien les classes. C'est ce que fait essentiellement le SVM - une simple séparation des classes. Le SVM, essaie de trouver une ligne/hyperplan (dans un espace multidimensionnel) qui sépare ces deux classes. Ensuite, il classe le nouveau point selon qu'il se trouve du côté positif ou négatif de l'hyperplan en fonction des classes à prédire.\n","La régression par vecteurs de support (SVR) utilise le même principe que le SVM, mais pour les problèmes de régression\n","Considérons que ces deux lignes rouges comme la limite de décision et la ligne verte comme l'hyperplan. Notre objectif, lorsque nous avançons avec le SVR, est de considérer essentiellement les points qui sont à l'intérieur de la ligne de limite de décision. Notre ligne de meilleur ajustement est l'hyperplan qui a un nombre maximum de points.\n","La première chose que nous allons comprendre est ce qu'est la frontière de décision (la dangereuse ligne rouge ci-dessus !). Considérez que ces lignes se trouvent à une distance quelconque, disons 'a', de l'hyperplan. Ce sont donc les lignes que nous traçons à la distance '+a' et '-a' de l'hyperplan\n","Le perceptron multicouche (MLP) est un complément du réseau neuronal à action directe. Il se compose de trois types de couches : la couche d'entrée, la couche de sortie et la couche cachée, comme le montre la figure. La couche d'entrée reçoit le signal d'entrée à traiter. La tâche requise, telle que la prédiction et la classification, est exécutée par la couche de sortie. Un nombre arbitraire de couches cachées, placées entre la couche d'entrée et la couche de sortie, constitue le véritable moteur de calcul du MLP. Comme dans le cas d'un réseau de type feed forward, dans un MLP, les données circulent dans le sens direct de la couche d'entrée à la couche de sortie. Les neurones du MLP sont formés avec l'algorithme d'apprentissage par rétro propagation. Les MLP sont conçus pour approximer toute fonction continue et peuvent résoudre des problèmes qui ne sont pas linéairement séparables. Les principaux cas d'utilisation des MLP sont la classification, la reconnaissance, la prédiction et\n","l'approximation des formes.\n","Couche d'entrée Cette couche accepte les caractéristiques d'entrée. Elle fournit au réseau des informations provenant du monde extérieur. Aucun calcul n'est effectué dans cette couche, les nœuds transmettent simplement les informations (caractéristiques) à la couche cachée.\n","Couche cachée Les nœuds de cette couche ne sont pas exposés au monde extérieur, ils font partie de l'abstraction fournie par tout réseau neuronal. La couche cachée effectue\n","toutes sortes de calculs sur les caractéristiques entrées par la couche d'entrée et transfère le résultat à la couche de sortie.\n","Couche de sortie : Cette couche transmet les informations apprises par le réseau au monde extérieur.\n","Fonction d’activation La fonction d'activation décide si un neurone doit être activé ou non en calculant la somme pondérée et en y ajoutant un biais. Le but de la fonction d'activation est d'introduire une non- linéarité dans la sortie d'un neurone.\n","Nous savons qu'un réseau neuronal possède des neurones qui fonctionnent en correspondance avec un poids, un biais et leur fonction d'activation respective. Dans un réseau neuronal, nous mettons à jour les poids et les biais des neurones sur la base de l'erreur à la sortie. Ce processus est connu sous le nom de rétro propagation. Les fonctions d'activation rendent la rétro propagation possible puisque les gradients sont fournis avec l'erreur pour mettre à jour les poids et les biais.\n","Exemples de fonction d’activation : « Sigmoid - Tanh - RELU - Softmax – Linear »\n","Pourquoi la fonction d’activation\n","Un réseau neuronal sans fonction d'activation est essentiellement un simple modèle de régression linéaire. La fonction d'activation effectue la transformation non linéaire de l'entrée, ce qui le rend capable d'apprendre et d'exécuter des tâches plus complexes.\n","L'analyse de régression est une technique de modélisation prédictive qui estime la relation entre deux ou plusieurs variables, il se concentre sur la relation entre une variable dépendante (cible) et une ou plusieurs variables indépendantes (prédicteurs). Ici, la variable dépendante est supposée être l'effet de la ou des variables indépendantes. La valeur des prédicteurs est utilisée pour estimer ou prédire la valeur probable de la variable cible.\n","Par exemple, pour décrire la relation entre la consommation de diesel et la production industrielle, si l'on suppose que la \"consommation de diesel\" est l'effet de la \"production industrielle\", nous pouvons effectuer une analyse de régression pour prédire la valeur de la \"consommation de diesel\" pour une valeur spécifique de la \"production industrielle\".\n","Etapes pour effectuer une régression linéaire Supposer une relation mathématique entre la cible et le(s) prédicteur(s). \"La relation peut être une ligne droite (régression linéaire) ou une courbe polynomiale (régression polynomiale) ou une relation non linéaire (régression non linéaire)\". Créez un diagramme de dispersion de la variable cible et de la variable prédictive (méthode la plus simple et la plus populaire).\n","En générale, l'analyse de régression comprend l'ensemble du processus d'identification de la cible et des prédicteurs, la recherche de la relation, l'estimation des coefficients, la recherche des valeurs prédites de la cible, et enfin l'évaluation de la précision de la relation ajustée.\n","Avantages de l'utilisation de l'analyse de régression La régression explore les relations significatives entre la variable dépendante et la variable indépendante Indique la force de l'impact de plusieurs variables indépendantes sur une variable dépendante Permet de comparer l'effet des mesures de variables sur différentes échelles et peut considérer des variables nominales, d'intervalle ou catégoriques pour l'analyse Régression linéaire : La régression linéaire est l'une des techniques de modélisation prédictive les plus couramment utilisées. Elle est représentée par une équation 𝑌 = 𝑎 +\n","𝑏𝑋 + 𝑒, où a est l'ordonnée à l'origine, b est la pente de la ligne et e est le terme d'erreur. Cette équation peut être utilisée pour prédire la valeur d'une variable cible en fonction d'une ou plusieurs variables prédicteurs données Logistique Régression: La régression logistique est utilisée pour expliquer la relation entre une variable binaire dépendante et une ou plusieurs variables indépendantes de niveau nominal, ordinal, intervalle ou ratio Régression polynomiale : Une équation de régression est une équation de régression polynomiale si la puissance de la variable indépendante est supérieure à 1. L'équation ci-dessous représente une équation polynomiale. 𝑌 = 𝑎 + 𝑏𝑋 + 𝑐𝑋2. Dans cette technique de régression, la ligne de meilleur ajustement n'est pas une ligne droite. Il s'agit plutôt d'une courbe qui s'adapte aux points de données.\n","\"\"\"\n","corpus = data.lower().split(\".\")\n","\n","\n","tokenizer.fit_on_texts(corpus)\n","total_words = len(tokenizer.word_index) + 1\n","\n","# create input sequences using list of tokens\n","input_sequences = []\n","for line in corpus:\n","\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n","\tfor i in range(1, len(token_list)):\n","\t\tn_gram_sequence = token_list[:i+1]\n","\t\tinput_sequences.append(n_gram_sequence)\n","\n","\n","# pad sequences \n","max_sequence_len = max([len(x) for x in input_sequences])\n","input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n","\n","# create predictors and label\n","predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n","\n","label = ku.to_categorical(label, num_classes=total_words)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading...\n","From: https://drive.google.com/uc?id=108jAePKK4R3BVYBbYJZ32JWUwxeMg20K\n","To: /content/sonnets.txt\n","\r  0% 0.00/93.6k [00:00<?, ?B/s]\r100% 93.6k/93.6k [00:00<00:00, 37.8MB/s]\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VVDTqGb1bOxn","outputId":"5f58d295-99c5-4802-c6ab-68389d752e3a"},"source":["### \n","\n","model = Sequential()\n","model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))#  Embedding Layer\n","model.add(Bidirectional(LSTM(40, return_sequences = True)))# LSTM Layer)\n","model.add(Bidirectional(LSTM(20)))# LSTM Layer)\n","\n","\n","model.add(Dense(total_words, activation='relu'))\n","model.add(Dense(total_words, activation='softmax'))\n","# optimizer\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","### \n","\n","print(model.summary())"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 83, 100)           111400    \n","                                                                 \n"," bidirectional (Bidirectiona  (None, 83, 80)           45120     \n"," l)                                                              \n","                                                                 \n"," bidirectional_1 (Bidirectio  (None, 40)               16160     \n"," nal)                                                            \n","                                                                 \n"," dense (Dense)               (None, 1114)              45674     \n","                                                                 \n"," dense_1 (Dense)             (None, 1114)              1242110   \n","                                                                 \n","=================================================================\n","Total params: 1,460,464\n","Trainable params: 1,460,464\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bPrOFdabcYSv","outputId":"651d303b-0fe9-48b5-cb40-6a11a9e83eaa"},"source":["history = model.fit(predictors, label, epochs=150, verbose=1)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/150\n","135/135 [==============================] - 28s 151ms/step - loss: 6.3127 - accuracy: 0.0520\n","Epoch 2/150\n","135/135 [==============================] - 20s 150ms/step - loss: 5.8846 - accuracy: 0.0576\n","Epoch 3/150\n","135/135 [==============================] - 20s 151ms/step - loss: 5.6394 - accuracy: 0.0578\n","Epoch 4/150\n","135/135 [==============================] - 20s 149ms/step - loss: 5.3580 - accuracy: 0.0734\n","Epoch 5/150\n","135/135 [==============================] - 20s 149ms/step - loss: 5.0827 - accuracy: 0.0836\n","Epoch 6/150\n","135/135 [==============================] - 20s 151ms/step - loss: 4.8173 - accuracy: 0.0934\n","Epoch 7/150\n","135/135 [==============================] - 20s 150ms/step - loss: 4.5334 - accuracy: 0.1110\n","Epoch 8/150\n","135/135 [==============================] - 20s 149ms/step - loss: 4.2309 - accuracy: 0.1319\n","Epoch 9/150\n","135/135 [==============================] - 20s 150ms/step - loss: 3.9360 - accuracy: 0.1570\n","Epoch 10/150\n","135/135 [==============================] - 20s 151ms/step - loss: 3.6695 - accuracy: 0.1854\n","Epoch 11/150\n","135/135 [==============================] - 20s 150ms/step - loss: 3.4187 - accuracy: 0.2193\n","Epoch 12/150\n","135/135 [==============================] - 20s 151ms/step - loss: 3.1777 - accuracy: 0.2590\n","Epoch 13/150\n","135/135 [==============================] - 20s 150ms/step - loss: 2.9454 - accuracy: 0.2906\n","Epoch 14/150\n","135/135 [==============================] - 20s 150ms/step - loss: 2.7219 - accuracy: 0.3319\n","Epoch 15/150\n","135/135 [==============================] - 20s 150ms/step - loss: 2.5056 - accuracy: 0.3756\n","Epoch 16/150\n","135/135 [==============================] - 20s 150ms/step - loss: 2.3112 - accuracy: 0.4098\n","Epoch 17/150\n","135/135 [==============================] - 20s 150ms/step - loss: 2.1118 - accuracy: 0.4576\n","Epoch 18/150\n","135/135 [==============================] - 20s 151ms/step - loss: 1.9265 - accuracy: 0.4864\n","Epoch 19/150\n","135/135 [==============================] - 20s 151ms/step - loss: 1.7723 - accuracy: 0.5317\n","Epoch 20/150\n","135/135 [==============================] - 20s 149ms/step - loss: 1.6348 - accuracy: 0.5700\n","Epoch 21/150\n","135/135 [==============================] - 20s 151ms/step - loss: 1.4858 - accuracy: 0.6014\n","Epoch 22/150\n","135/135 [==============================] - 20s 150ms/step - loss: 1.3555 - accuracy: 0.6341\n","Epoch 23/150\n","135/135 [==============================] - 20s 150ms/step - loss: 1.2539 - accuracy: 0.6697\n","Epoch 24/150\n","135/135 [==============================] - 20s 150ms/step - loss: 1.1610 - accuracy: 0.6880\n","Epoch 25/150\n","135/135 [==============================] - 20s 150ms/step - loss: 1.0440 - accuracy: 0.7194\n","Epoch 26/150\n","135/135 [==============================] - 20s 148ms/step - loss: 0.9529 - accuracy: 0.7429\n","Epoch 27/150\n","135/135 [==============================] - 20s 149ms/step - loss: 0.8801 - accuracy: 0.7689\n","Epoch 28/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.8095 - accuracy: 0.7786\n","Epoch 29/150\n","135/135 [==============================] - 20s 151ms/step - loss: 0.7318 - accuracy: 0.8060\n","Epoch 30/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.6738 - accuracy: 0.8211\n","Epoch 31/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.6125 - accuracy: 0.8418\n","Epoch 32/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.5920 - accuracy: 0.8432\n","Epoch 33/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.5091 - accuracy: 0.8718\n","Epoch 34/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.5002 - accuracy: 0.8725\n","Epoch 35/150\n","135/135 [==============================] - 20s 151ms/step - loss: 0.4695 - accuracy: 0.8736\n","Epoch 36/150\n","135/135 [==============================] - 20s 152ms/step - loss: 0.4410 - accuracy: 0.8825\n","Epoch 37/150\n","135/135 [==============================] - 20s 151ms/step - loss: 0.3807 - accuracy: 0.9031\n","Epoch 38/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.3498 - accuracy: 0.9066\n","Epoch 39/150\n","135/135 [==============================] - 20s 151ms/step - loss: 0.3032 - accuracy: 0.9247\n","Epoch 40/150\n","135/135 [==============================] - 21s 152ms/step - loss: 0.2814 - accuracy: 0.9292\n","Epoch 41/150\n","135/135 [==============================] - 20s 151ms/step - loss: 0.2655 - accuracy: 0.9294\n","Epoch 42/150\n","135/135 [==============================] - 20s 151ms/step - loss: 0.2517 - accuracy: 0.9389\n","Epoch 43/150\n","135/135 [==============================] - 20s 151ms/step - loss: 0.2405 - accuracy: 0.9405\n","Epoch 44/150\n","135/135 [==============================] - 21s 152ms/step - loss: 0.2495 - accuracy: 0.9375\n","Epoch 45/150\n","135/135 [==============================] - 20s 151ms/step - loss: 0.2357 - accuracy: 0.9440\n","Epoch 46/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.2161 - accuracy: 0.9447\n","Epoch 47/150\n","135/135 [==============================] - 20s 151ms/step - loss: 0.2834 - accuracy: 0.9233\n","Epoch 48/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.2869 - accuracy: 0.9220\n","Epoch 49/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.2560 - accuracy: 0.9324\n","Epoch 50/150\n","135/135 [==============================] - 20s 149ms/step - loss: 0.2574 - accuracy: 0.9305\n","Epoch 51/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.2144 - accuracy: 0.9443\n","Epoch 52/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.1578 - accuracy: 0.9584\n","Epoch 53/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.1566 - accuracy: 0.9587\n","Epoch 54/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.1457 - accuracy: 0.9614\n","Epoch 55/150\n","135/135 [==============================] - 20s 149ms/step - loss: 0.1285 - accuracy: 0.9645\n","Epoch 56/150\n","135/135 [==============================] - 20s 149ms/step - loss: 0.1204 - accuracy: 0.9677\n","Epoch 57/150\n","135/135 [==============================] - 20s 148ms/step - loss: 0.1167 - accuracy: 0.9668\n","Epoch 58/150\n","135/135 [==============================] - 20s 148ms/step - loss: 0.1130 - accuracy: 0.9679\n","Epoch 59/150\n","135/135 [==============================] - 20s 149ms/step - loss: 0.1145 - accuracy: 0.9672\n","Epoch 60/150\n","135/135 [==============================] - 20s 149ms/step - loss: 0.1167 - accuracy: 0.9668\n","Epoch 61/150\n","135/135 [==============================] - 20s 149ms/step - loss: 0.1146 - accuracy: 0.9670\n","Epoch 62/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.1803 - accuracy: 0.9535\n","Epoch 63/150\n","135/135 [==============================] - 20s 149ms/step - loss: 0.5977 - accuracy: 0.8239\n","Epoch 64/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.4785 - accuracy: 0.8504\n","Epoch 65/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.2583 - accuracy: 0.9238\n","Epoch 66/150\n","135/135 [==============================] - 20s 148ms/step - loss: 0.1383 - accuracy: 0.9614\n","Epoch 67/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.1091 - accuracy: 0.9679\n","Epoch 68/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.1003 - accuracy: 0.9689\n","Epoch 69/150\n","135/135 [==============================] - 20s 149ms/step - loss: 0.0994 - accuracy: 0.9682\n","Epoch 70/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.0958 - accuracy: 0.9691\n","Epoch 71/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.0936 - accuracy: 0.9703\n","Epoch 72/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.0954 - accuracy: 0.9689\n","Epoch 73/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.0955 - accuracy: 0.9696\n","Epoch 74/150\n","135/135 [==============================] - 20s 149ms/step - loss: 0.0944 - accuracy: 0.9698\n","Epoch 75/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.0925 - accuracy: 0.9684\n","Epoch 76/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.0957 - accuracy: 0.9686\n","Epoch 77/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.0999 - accuracy: 0.9689\n","Epoch 78/150\n","135/135 [==============================] - 20s 149ms/step - loss: 0.1182 - accuracy: 0.9645\n","Epoch 79/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.1513 - accuracy: 0.9531\n","Epoch 80/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.2563 - accuracy: 0.9226\n","Epoch 81/150\n","135/135 [==============================] - 20s 149ms/step - loss: 0.3449 - accuracy: 0.8906\n","Epoch 82/150\n","135/135 [==============================] - 20s 149ms/step - loss: 0.2458 - accuracy: 0.9261\n","Epoch 83/150\n","135/135 [==============================] - 20s 149ms/step - loss: 0.1435 - accuracy: 0.9584\n","Epoch 84/150\n","135/135 [==============================] - 20s 148ms/step - loss: 0.1098 - accuracy: 0.9631\n","Epoch 85/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.0978 - accuracy: 0.9677\n","Epoch 86/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.0912 - accuracy: 0.9686\n","Epoch 87/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.0871 - accuracy: 0.9707\n","Epoch 88/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.0901 - accuracy: 0.9668\n","Epoch 89/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.0844 - accuracy: 0.9705\n","Epoch 90/150\n","135/135 [==============================] - 20s 148ms/step - loss: 0.0846 - accuracy: 0.9710\n","Epoch 91/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.0858 - accuracy: 0.9710\n","Epoch 92/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.0816 - accuracy: 0.9710\n","Epoch 93/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.0844 - accuracy: 0.9689\n","Epoch 94/150\n","135/135 [==============================] - 20s 151ms/step - loss: 0.0840 - accuracy: 0.9703\n","Epoch 95/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.0842 - accuracy: 0.9693\n","Epoch 96/150\n","135/135 [==============================] - 20s 148ms/step - loss: 0.0845 - accuracy: 0.9693\n","Epoch 97/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.0949 - accuracy: 0.9679\n","Epoch 98/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.1028 - accuracy: 0.9642\n","Epoch 99/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.1994 - accuracy: 0.9408\n","Epoch 100/150\n","135/135 [==============================] - 20s 151ms/step - loss: 0.4453 - accuracy: 0.8657\n","Epoch 101/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.2753 - accuracy: 0.9110\n","Epoch 102/150\n","135/135 [==============================] - 20s 148ms/step - loss: 0.1634 - accuracy: 0.9496\n","Epoch 103/150\n","135/135 [==============================] - 20s 149ms/step - loss: 0.1050 - accuracy: 0.9661\n","Epoch 104/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.0840 - accuracy: 0.9682\n","Epoch 105/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.0801 - accuracy: 0.9693\n","Epoch 106/150\n","135/135 [==============================] - 20s 149ms/step - loss: 0.0798 - accuracy: 0.9710\n","Epoch 107/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.0791 - accuracy: 0.9703\n","Epoch 108/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.0765 - accuracy: 0.9700\n","Epoch 109/150\n","135/135 [==============================] - 20s 151ms/step - loss: 0.0792 - accuracy: 0.9703\n","Epoch 110/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.0785 - accuracy: 0.9710\n","Epoch 111/150\n","135/135 [==============================] - 20s 148ms/step - loss: 0.0793 - accuracy: 0.9691\n","Epoch 112/150\n","135/135 [==============================] - 20s 151ms/step - loss: 0.0780 - accuracy: 0.9717\n","Epoch 113/150\n","135/135 [==============================] - 20s 151ms/step - loss: 0.0784 - accuracy: 0.9696\n","Epoch 114/150\n","135/135 [==============================] - 20s 149ms/step - loss: 0.0784 - accuracy: 0.9705\n","Epoch 115/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.0776 - accuracy: 0.9703\n","Epoch 116/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.0779 - accuracy: 0.9693\n","Epoch 117/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.0781 - accuracy: 0.9710\n","Epoch 118/150\n","135/135 [==============================] - 20s 148ms/step - loss: 0.0768 - accuracy: 0.9691\n","Epoch 119/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.0770 - accuracy: 0.9717\n","Epoch 120/150\n","135/135 [==============================] - 20s 148ms/step - loss: 0.0768 - accuracy: 0.9698\n","Epoch 121/150\n","135/135 [==============================] - 20s 149ms/step - loss: 0.0765 - accuracy: 0.9700\n","Epoch 122/150\n","135/135 [==============================] - 20s 147ms/step - loss: 0.0764 - accuracy: 0.9698\n","Epoch 123/150\n","135/135 [==============================] - 20s 149ms/step - loss: 0.2614 - accuracy: 0.9206\n","Epoch 124/150\n","135/135 [==============================] - 20s 149ms/step - loss: 0.5180 - accuracy: 0.8462\n","Epoch 125/150\n","135/135 [==============================] - 20s 149ms/step - loss: 0.2326 - accuracy: 0.9247\n","Epoch 126/150\n","135/135 [==============================] - 20s 147ms/step - loss: 0.1184 - accuracy: 0.9589\n","Epoch 127/150\n","135/135 [==============================] - 20s 149ms/step - loss: 0.0881 - accuracy: 0.9689\n","Epoch 128/150\n","135/135 [==============================] - 20s 149ms/step - loss: 0.0775 - accuracy: 0.9714\n","Epoch 129/150\n","135/135 [==============================] - 20s 149ms/step - loss: 0.0734 - accuracy: 0.9707\n","Epoch 130/150\n","135/135 [==============================] - 20s 148ms/step - loss: 0.0752 - accuracy: 0.9703\n","Epoch 131/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.0736 - accuracy: 0.9714\n","Epoch 132/150\n","135/135 [==============================] - 20s 149ms/step - loss: 0.0720 - accuracy: 0.9714\n","Epoch 133/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.0741 - accuracy: 0.9705\n","Epoch 134/150\n","135/135 [==============================] - 20s 149ms/step - loss: 0.0727 - accuracy: 0.9717\n","Epoch 135/150\n","135/135 [==============================] - 20s 148ms/step - loss: 0.0728 - accuracy: 0.9719\n","Epoch 136/150\n","135/135 [==============================] - 20s 149ms/step - loss: 0.0728 - accuracy: 0.9712\n","Epoch 137/150\n","135/135 [==============================] - 20s 151ms/step - loss: 0.0750 - accuracy: 0.9703\n","Epoch 138/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.0731 - accuracy: 0.9698\n","Epoch 139/150\n","135/135 [==============================] - 20s 151ms/step - loss: 0.0733 - accuracy: 0.9712\n","Epoch 140/150\n","135/135 [==============================] - 21s 152ms/step - loss: 0.0742 - accuracy: 0.9710\n","Epoch 141/150\n","135/135 [==============================] - 20s 152ms/step - loss: 0.0715 - accuracy: 0.9710\n","Epoch 142/150\n","135/135 [==============================] - 20s 151ms/step - loss: 0.0721 - accuracy: 0.9696\n","Epoch 143/150\n","135/135 [==============================] - 20s 151ms/step - loss: 0.0709 - accuracy: 0.9710\n","Epoch 144/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.0728 - accuracy: 0.9719\n","Epoch 145/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.0747 - accuracy: 0.9707\n","Epoch 146/150\n","135/135 [==============================] - 20s 151ms/step - loss: 0.0714 - accuracy: 0.9712\n","Epoch 147/150\n","135/135 [==============================] - 20s 151ms/step - loss: 0.0724 - accuracy: 0.9717\n","Epoch 148/150\n","135/135 [==============================] - 20s 150ms/step - loss: 0.0722 - accuracy: 0.9700\n","Epoch 149/150\n","135/135 [==============================] - 20s 151ms/step - loss: 0.0710 - accuracy: 0.9719\n","Epoch 150/150\n","135/135 [==============================] - 20s 151ms/step - loss: 0.0728 - accuracy: 0.9712\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UoPHfe32cbB-","outputId":"6ca02c55-b027-4868-ae20-36be96f6acb3"},"source":["seed_text = \"Le système\"\n","next_words = 30\n","\n","for _ in range(next_words):\n","  token_list = tokenizer.texts_to_sequences([seed_text])[0]\n","  token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n","  predicted = model.predict(token_list, verbose=0)\n","  predicted_class = np.argmax(predicted)\n","  output_word = \"\"\n","  for word, index in tokenizer.word_index.items():\n","    if index == predicted_class:\n","      output_word = word\n","      #break\n","      seed_text += \" \" + output_word\n","  print(seed_text)\n","\n"," "],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Le système automatisé\n","Le système automatisé qui\n","Le système automatisé qui est\n","Le système automatisé qui est fourni\n","Le système automatisé qui est fourni par\n","Le système automatisé qui est fourni par l'adas\n","Le système automatisé qui est fourni par l'adas au\n","Le système automatisé qui est fourni par l'adas au véhicule\n","Le système automatisé qui est fourni par l'adas au véhicule a\n","Le système automatisé qui est fourni par l'adas au véhicule a prouvé\n","Le système automatisé qui est fourni par l'adas au véhicule a prouvé qu'il\n","Le système automatisé qui est fourni par l'adas au véhicule a prouvé qu'il permettait\n","Le système automatisé qui est fourni par l'adas au véhicule a prouvé qu'il permettait de\n","Le système automatisé qui est fourni par l'adas au véhicule a prouvé qu'il permettait de réduire\n","Le système automatisé qui est fourni par l'adas au véhicule a prouvé qu'il permettait de réduire le\n","Le système automatisé qui est fourni par l'adas au véhicule a prouvé qu'il permettait de réduire le nombre\n","Le système automatisé qui est fourni par l'adas au véhicule a prouvé qu'il permettait de réduire le nombre de\n","Le système automatisé qui est fourni par l'adas au véhicule a prouvé qu'il permettait de réduire le nombre de décès\n","Le système automatisé qui est fourni par l'adas au véhicule a prouvé qu'il permettait de réduire le nombre de décès et\n","Le système automatisé qui est fourni par l'adas au véhicule a prouvé qu'il permettait de réduire le nombre de décès et des\n","Le système automatisé qui est fourni par l'adas au véhicule a prouvé qu'il permettait de réduire le nombre de décès et des dégâts\n","Le système automatisé qui est fourni par l'adas au véhicule a prouvé qu'il permettait de réduire le nombre de décès et des dégâts d’une\n","Le système automatisé qui est fourni par l'adas au véhicule a prouvé qu'il permettait de réduire le nombre de décès et des dégâts d’une manière\n","Le système automatisé qui est fourni par l'adas au véhicule a prouvé qu'il permettait de réduire le nombre de décès et des dégâts d’une manière remarquable\n","Le système automatisé qui est fourni par l'adas au véhicule a prouvé qu'il permettait de réduire le nombre de décès et des dégâts d’une manière remarquable sur\n","Le système automatisé qui est fourni par l'adas au véhicule a prouvé qu'il permettait de réduire le nombre de décès et des dégâts d’une manière remarquable sur les\n","Le système automatisé qui est fourni par l'adas au véhicule a prouvé qu'il permettait de réduire le nombre de décès et des dégâts d’une manière remarquable sur les routes\n","Le système automatisé qui est fourni par l'adas au véhicule a prouvé qu'il permettait de réduire le nombre de décès et des dégâts d’une manière remarquable sur les routes en\n","Le système automatisé qui est fourni par l'adas au véhicule a prouvé qu'il permettait de réduire le nombre de décès et des dégâts d’une manière remarquable sur les routes en minimisant\n","Le système automatisé qui est fourni par l'adas au véhicule a prouvé qu'il permettait de réduire le nombre de décès et des dégâts d’une manière remarquable sur les routes en minimisant les\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xboR_C-1lrQL","outputId":"17ab5558-5359-4522-98c3-9c3fe3019e92"},"source":["print(\"\\nle texte final:\", seed_text)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","le texte final: Le système automatisé qui est fourni par l'adas au véhicule a prouvé qu'il permettait de réduire le nombre de décès et des dégâts d’une manière remarquable sur les routes en minimisant les\n"]}]}]}
